# æ€»è§ˆå›¾

## A Map of the Territory

> you must have a map, no matter how rough. Otherwise you wander all over the place. In The Lord of the Rings I never made anyone go farther than he could on a given day.
>
> <p align="right">â€”â€” J.R.R. Tolkien</p>

é¢†åœŸå›¾

>ä¸ç®¡æœ‰å¤šä¹ˆç²—ç³™ï¼Œä½ éƒ½å¿…é¡»æ‹¥æœ‰ä¸€å¹…é¢†åœŸåœ°å›¾ï¼Œå¦åˆ™ä½ å°†åˆ°å¤„æ¸¸è¡ã€‚åœ¨ã€ŠæŒ‡ç¯ç‹ã€‹ä¸­ï¼Œæˆ‘ä»æ¥éƒ½æ²¡æœ‰è®©ä»»ä½•äººä¸€å¤©å†…èµ°çš„æ›´è¿œã€‚
>
> <p align="right">â€”â€” J.R.R. Tolkien</p>

We donâ€™t want to wander all over the place, so before we set off, letâ€™s scan the territory charted by previous language implementers. It will help us understand where we are going and the alternate routes others have taken.

First, let me establish a shorthand.  Much of this book is about a languageâ€™s implementation, which is distinct from the language itself in some sort of Platonic ideal form. Things like â€œstackâ€, â€œbytecodeâ€, and â€œrecursive descentâ€, are nuts and bolts one particular implementation might use. From the userâ€™s perspective, as long as the resulting contraption faithfully follows the languageâ€™s specification, itâ€™s all implementation detail.

Weâ€™re going to spend a lot of time on those details, so if I have to write â€œlanguage implementationâ€ every single time I mention them, Iâ€™ll wear my fingers off. Instead, Iâ€™ll use â€œlanguageâ€ to refer to either a language or an implementation of it, or both, unless the distinction matters.

æˆ‘ä»¬ä¸æƒ³åˆ°å¤„çé€›ï¼Œæ‰€ä»¥åœ¨å‡ºå‘ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæµè§ˆä¸€ä¸‹ä»¥å‰çš„è¯­è¨€å®ç°è€…ç»˜åˆ¶çš„é¢†åœŸå›¾ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬ç†è§£æˆ‘ä»¬çš„ç›®æ ‡ï¼Œäº†è§£æ›´å¤šçš„æ›¿ä»£è·¯å¾„ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªæ¦‚è§ˆã€‚æœ¬ä¹¦çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½æ˜¯å¦‚ä½•å®ç°ä¸€é—¨è¯­è¨€ï¼Œè¿™å’Œä¸€é—¨è¯­è¨€æœ¬èº«æŸæ‹‰å›¾å¼çš„ç†æƒ³æ¦‚å¿µæœ‰æ‰€ä¸åŒã€‚ åƒæ˜¯æ ˆã€å­—èŠ‚ç ã€é€’å½’ä¸‹é™ç­‰ä¸œè¥¿ï¼Œæ˜¯ä¸€ä¸ªç‰¹å®šå®ç°å¯èƒ½ä¼šç”¨åˆ°çš„å…·ä½“ç»†èŠ‚ã€‚ ä»ç”¨æˆ·çš„è§’åº¦ï¼Œåªè¦ç”Ÿæˆçš„å†…å®¹è¿˜éµå¾ªç€è¯­è¨€çš„è§„èŒƒï¼Œå®ƒå°±æ˜¯æ‰€æœ‰çš„å®ç°ç»†èŠ‚ã€‚

æˆ‘ä»¬å°†åœ¨è¿™äº›ç»†èŠ‚ä¸ŠèŠ±è´¹å¤§é‡æ—¶é—´ï¼Œå› æ­¤ï¼Œå¦‚æœæ¯æ¬¡æåˆ°è¿™äº›ç»†èŠ‚ï¼Œæˆ‘éƒ½è¦åŠ ä¸Šè¯­è¨€å®ç°è¯´æ˜ï¼Œé‚£ä¹ˆæˆ‘ä¼šç´¯æ™•çš„ã€‚æ‰€ä»¥ï¼Œæˆ‘å°†ä½¿ç”¨è¯­è¨€æ¥è¡¨ç¤ºä¸€é—¨è¯­è¨€æˆ–è€…è¿™é—¨è¯­è¨€çš„å®ç°ï¼Œæˆ–è€…ä¸¤è€…ï¼Œé™¤éä¸¤è€…çš„åŒºåˆ«éå¸¸é‡è¦ã€‚


## ä¸€ã€The Parts of a Language

è¯­è¨€çš„ç»„æˆéƒ¨åˆ†

Engineers have been building programming languages since the Dark Ages of computing. As soon as we could talk to computers, we discovered doing so was too hard, and we enlisted their help. I find it fascinating that even though todayâ€™s machines are literally a million times faster and have orders of magnitude more storage, the way we build programming languages is virtually unchanged.

Though the area explored by language designers is vast, the trails theyâ€™ve carved through it are few. Not every language takes the exact same pathâ€”some take a shortcut or twoâ€”but otherwise they are reassuringly similar, from Rear Admiral Grace Hopperâ€™s first COBOL compiler all the way to some hot, new, transpile-to-JavaScript language whose â€œdocumentationâ€ consists entirely of a single, poorly edited README in a Git repository somewhere.

è‡ªè®¡ç®—çš„é»‘æš—æ—¶ä»£ä»¥æ¥ï¼Œå·¥ç¨‹å¸ˆä»¬ä¸€ç›´åœ¨æ„å»ºç¼–ç¨‹è¯­è¨€ã€‚å½“æˆ‘ä»¬å¯ä»¥ä¸ç”µè„‘äº¤æµæ—¶å€™ï¼Œæˆ‘ä»¬å‘ç°è¿™æ ·åšå¤ªéš¾äº†ï¼Œéœ€è¦ç”µè„‘çš„å¸®åŠ©ã€‚æˆ‘å‘ç°ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼Œå³ä½¿ä»Šå¤©çš„æœºå™¨è¿è¡Œé€Ÿåº¦å¿«äº†æ•°ç™¾ä¸‡å€ï¼Œå­˜å‚¨é‡ä¹Ÿå¢åŠ äº†å‡ ä¸ªæ•°é‡çº§ï¼Œä½†æ˜¯æˆ‘ä»¬æ„å»ºç¼–ç¨‹è¯­è¨€çš„æ–¹å¼å‡ ä¹æ²¡æœ‰ä»»ä½•æ”¹å˜ã€‚

è™½ç„¶ï¼Œè¯­è¨€è®¾è®¡è€…æ¢ç´¢çš„é¢†åŸŸéå¸¸å¤§ï¼Œä½†æ˜¯ä»–ä»¬åœ¨å…¶ä¸­å¼€è¾Ÿçš„é“è·¯å´éå¸¸å°‘ã€‚å¹¶ä¸æ˜¯æ‰€æœ‰çš„è¯­è¨€éƒ½èµ°ç›¸åŒçš„è·¯å¾„ï¼Œæœ‰äº›è¯­è¨€çš„å®ç°ï¼Œä¼šèµ°ä¸€ã€ä¸¤æ¡æ·å¾„ã€‚ä½†æ˜¯ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œå®ƒä»¬éƒ½æ˜¯ç›¸ä¼¼çš„ã€‚ä»ç¬¬ä¸€ä¸ª COBOLç¼–è¯‘å™¨åˆ°ç°åœ¨æœ€æ–°çš„å¯ä»¥è½¬æ¢ä¸º JavaScriptçš„è¯­è¨€ï¼Œåœ¨å®ƒä»¬ gitä»“åº“READMEæ–‡ä»¶ä¸­çš„æè¿°éƒ½æ˜¯ç›¸ä¼¼çš„ã€‚

>There are certainly dead ends, sad little cul-de-sacs of CS papers with zero citations and now-forgotten optimizations that only made sense when memory was measured in individual bytes.
>
>æ¯«æ— ç–‘é—®ï¼Œè®¡ç®—æœºç§‘å­¦è®ºæ–‡å­˜åœ¨ä¸€äº›æ­»èƒ¡åŒã€‚è¿™äº›è®ºæ–‡ç°åœ¨å·²ç»æ²¡æœ‰äººå¼•ç”¨ï¼Œéƒ½æ˜¯åœ¨å†…å­˜éœ€è¦ä»¥ä¸€ä¸ªä¸€ä¸ªå­—èŠ‚æ¥è¡¡é‡æ—¶æœŸçš„ä¼˜åŒ–ä½¿ç”¨è®ºæ–‡ã€‚

I visualize the network of paths an implementation may choose as climbing a mountain. You start off at the bottom with the program as raw source text, literally just a string of characters. Each phase analyzes the program and transforms it to some higher-level representation where the semanticsâ€”what the author wants the computer to doâ€”become more apparent.

![a map of the territory](https://github.com/Kua-Fu/blog-book-images/blob/main/crafting-interpreters/mountain.png?raw=true)

Eventually we reach the peak. We have a birdâ€™s-eye view of the userâ€™s program and can see what their code means. We begin our descent down the other side of the mountain. We transform this highest-level representation down to successively lower-level forms to get closer and closer to something we know how to make the CPU actually execute.

æˆ‘æŠŠç¼–è¯‘é¢†åŸŸå›¾ï¼Œæƒ³è±¡ä¸ºä¸€å¹…åŒ…å«å¾ˆå¤šè·¯å¾„çš„çˆ¬å±±å›¾ã€‚ä»åº•éƒ¨å¼€å§‹ï¼Œä¸€å¼€å§‹åªæ˜¯ä¸€ä¸ªæ–‡æœ¬ï¼Œå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚ç»è¿‡ï¼Œæ¯ä¸ªåˆ†æé˜¶æ®µï¼Œéƒ½ä¼šç”Ÿæˆæ›´åŠ é«˜çº§çš„è¡¨ç¤ºï¼Œè®¾è®¡è€…å¸Œæœ›è®¡ç®—æœºæ‰§è¡Œçš„è¯­è¨€éƒ½æ›´åŠ æ˜ç¡®ã€‚

æœ€åï¼Œæˆ‘ä»¬çˆ¬ä¸Šäº†å±±é¡¶ã€‚æˆ‘ä»¬é¸Ÿç°å…¨å±€ï¼Œå¯ä»¥å¾—åˆ°ä½¿ç”¨è€…ç¼–å†™çš„ä»£ç å«ä¹‰ã€‚æˆ‘ä»¬ä»å±±çš„å¦ä¸€è¾¹å¼€å§‹ä¸‹å±±ï¼Œæˆ‘ä»¬å°†è¿ç»­å°†é«˜çº§åˆ«çš„è¡¨ç¤ºè½¬æ¢ä¸ºæ›´ä½çº§åˆ«çš„è¡¨ç¤ºï¼Œä»¥è¶Šæ¥è¶Šæ¥è¿‘è®¡ç®—æœº CPUæ‰§è¡Œçš„è¯­è¨€ã€‚

Letâ€™s trace through each of those trails and points of interest. Our journey begins on the left with the bare text of the userâ€™s source code:

![string](https://github.com/Kua-Fu/blog-book-images/blob/main/crafting-interpreters/string.png?raw=true)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿½è¸ªæ¯ä¸€æ¡è·¯å¾„ã€æ¯ä¸€ä¸ªåœç•™ç‚¹ï¼Œæˆ‘ä»¬çš„æ—…é€”ä»å·¦è¾¹å±±è„šå¼€å§‹ï¼ˆç”¨æˆ·æºä»£ç ï¼‰ã€‚

### 1.1 Scanning

æ‰«æ

The first step is scanning, also known as lexing, or (if youâ€™re trying to impress someone) lexical analysis. They all mean pretty much the same thing. I like â€œlexingâ€ because it sounds like something an evil supervillain would do, but Iâ€™ll use â€œscanningâ€ because it seems to be marginally more commonplace.

A scanner (or lexer) takes in the linear stream of characters and chunks them together into a series of something more akin to â€œwordsâ€. In programming languages, each of these words is called a token. Some tokens are single characters, like ( and ,. Others may be several characters long, like numbers (123), string literals ("hi!"), and identifiers (min).

Some characters in a source file donâ€™t actually mean anything. Whitespace is often insignificant, and comments, by definition, are ignored by the language. The scanner usually discards these, leaving a clean sequence of meaningful tokens.

![tokens](https://github.com/Kua-Fu/blog-book-images/blob/main/crafting-interpreters/tokens.png?raw=true)

ç¬¬ä¸€æ­¥æ˜¯æ‰«æï¼Œä¹Ÿç§°ä¸ºè¯æ³•ï¼Œå¦‚æœä½ æƒ³ç»™åˆ«äººç•™ä¸‹æ·±åˆ»å°è±¡ï¼Œè¿˜å¯ä»¥ç§°ä¸ºè¯æ³•åˆ†æã€‚ä»–ä»¬çš„å«ä¹‰éƒ½å·®ä¸å¤šï¼Œæˆ‘æ›´å–œæ¬¢è¯æ³•ï¼Œå› ä¸ºè¿™å¬èµ·æ¥åƒæ˜¯ä¸€ä¸ªæ¶ä½œå‰§ï¼Œä½†æ˜¯æ¥ä¸‹æ¥æˆ‘ä¼šä½¿ç”¨æ‰«æè¡¨ç¤ºè¿™ä¸ªè¿‡ç¨‹ï¼Œå› ä¸ºè¿™ç§è¯´æ³•æ›´åŠ å¸¸è§ã€‚

ä¸€ä¸ªæ‰«æå™¨ï¼Œæ¥æ”¶çº¿æ€§çš„å­—ç¬¦ä¸²ï¼Œå°†å®ƒä»¬åˆ†å—ä¸ºä¸€ä¸ªä¸ªå•è¯ï¼Œåœ¨ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œåˆ†æˆçš„å•è¯ç§°ä¸º token, ä¸€äº› token æ˜¯å•å­—ç¬¦ï¼Œä¾‹å¦‚: `(` `,` è¿˜æœ‰ä¸€äº›tokené•¿åº¦æ˜¯å¤šä¸ªå­—ç¬¦ï¼Œä¾‹å¦‚: æ•°å­— `123` ï¼Œå­—ç¬¦ä¸² `"hi!"` , æ ‡è¯†ç¬¦ `min`

æºæ–‡ä»¶ä¸­çš„æŸäº›å­—ç¬¦æ²¡æœ‰å®é™…æ„ä¹‰ã€‚ç©ºç™½å­—ç¬¦ï¼Œé€šå¸¸æ²¡æœ‰å®é™…æ„ä¹‰ï¼Œè¿˜æœ‰æ³¨é‡Šï¼Œæ ¹æ®è¯­è¨€å®šä¹‰ï¼Œæ³¨é‡Šä¼šè¢«è¯­è¨€å¿½ç•¥ã€‚æ‰«æå™¨é€šå¸¸ä¼šå¿½ç•¥è¿™äº›å†…å®¹ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå¹²å‡€çš„æœ‰æ„ä¹‰çš„token åºåˆ—ã€‚

### 1.2 Parsing

è§£æ

The next step is parsing. This is where our syntax gets a grammarâ€”the ability to compose larger expressions and statements out of smaller parts. Did you ever diagram sentences in English class? If so, youâ€™ve done what a parser does, except that English has thousands and thousands of â€œkeywordsâ€ and an overflowing cornucopia of ambiguity. Programming languages are much simpler.

A parser takes the flat sequence of tokens and builds a tree structure that mirrors the nested nature of the grammar. These trees have a couple of different namesâ€”parse tree or abstract syntax treeâ€”depending on how close to the bare syntactic structure of the source language they are.  In practice, language hackers usually call them syntax trees, ASTs, or often just trees.

![ast](https://github.com/Kua-Fu/blog-book-images/blob/main/crafting-interpreters/ast.png?raw=true)

ä¸‹ä¸€æ­¥æ˜¯è§£æï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è·å¾—è¯­æ³•çš„åœ°æ–¹ï¼Œè¯­æ³•å¯ä»¥å°†è¾ƒå°çš„éƒ¨åˆ†ç»„åˆæˆè¾ƒå¤§çš„è¡¨è¾¾å¼å’Œè¯­å¥ã€‚ä½ åœ¨è‹±è¯­è¯¾å ‚ä¸Šç”»è¿‡å¥å­å›¾å—ï¼Ÿå¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œä½ å·²ç»å®Œæˆäº†è§£æå™¨çš„å·¥ä½œã€‚é™¤äº†è‹±è¯­æœ‰æˆåƒä¸Šä¸‡ä¸ªå…³é”®è¯å’Œæ›´å¤šçš„æ­§ä¹‰ã€‚ç›¸è¾ƒè€Œè¨€ï¼Œç¼–ç¨‹è¯­è¨€å°±ç®€å•å¤ªå¤šäº†ã€‚

è§£æå™¨æ¥æ”¶token åºåˆ—ï¼Œç„¶åæ„å»ºå‡ºååº”è¯­æ³•åµŒå¥—æ€§è´¨çš„æ ‘ç»“æ„ã€‚è¿™äº›æ ‘æœ‰ä¸€äº›ä¸åŒçš„åç§°ï¼Œä¾‹å¦‚ï¼šåç§°è§£ææ ‘ï¼ŒæŠ½è±¡è¯­æ³•æ ‘ï¼Œå‘½åå–å†³äºè¿™äº›æ ‘å’Œæºè¯­è¨€çš„ç®€å•è¯­æ³•ç»“æ„çš„æ¥è¿‘ç¨‹åº¦ã€‚åœ¨å®è·µä¸­ï¼Œè¯­è¨€é«˜æ‰‹ç»å¸¸ç§°å®ƒä»¬ä¸ºè¯­æ³•æ ‘ï¼ŒASTæˆ–è€…é€šå¸¸å°±ç§°ä¸ºæ ‘ã€‚

Parsing has a long, rich history in computer science that is closely tied to the artificial intelligence community. Many of the techniques used today to parse programming languages were originally conceived to parse human languages by AI researchers who were trying to get computers to talk to us.

It turns out human languages were too messy for the rigid grammars those parsers could handle, but they were a perfect fit for the simpler artificial grammars of programming languages. Alas, we flawed humans still manage to use those simple grammars incorrectly, so the parserâ€™s job also includes letting us know when we do by reporting syntax errors.

è§£æåœ¨è®¡ç®—æœºç§‘å­¦ä¸­æ‹¥æœ‰æ‚ ä¹…ä¸°å¯Œçš„å†å²ï¼Œä¸äººå·¥æ™ºèƒ½é¢†åŸŸå¯†åˆ‡ç›¸å…³ã€‚ä»Šå¤©ç”¨äºè§£æç¼–ç¨‹è¯­è¨€çš„è®¸å¤šæŠ€æœ¯æœ€åˆæ˜¯ç”±äººå·¥æ™ºèƒ½ç ”ç©¶äººå‘˜æ„æ€çš„ï¼Œä»–ä»¬æœ€åˆè®¾æƒ³æ˜¯è®©è®¡ç®—æœºä¸äººå¯¹è¯äº¤æµã€‚

äº‹å®è¯æ˜ï¼Œå¯¹äºè§£æå™¨æ‰€èƒ½è§£æçš„è¯­æ³•è€Œè¨€ï¼Œäººç±»çš„è¯­è¨€å¤ªå¤æ‚äº†ï¼Œä½†æ˜¯è¿™äº›è§£æå™¨å´éå¸¸é€‚åˆç¼–ç¨‹è¯­è¨€ä¸­çš„äººç±»å®šä¹‰çš„è¯­æ³•è§„åˆ™ã€‚å“ï¼Œæˆ‘ä»¬è¿™äº›æ™®é€šçš„äººï¼Œåœ¨ä½¿ç”¨è¿™äº›ç®€å•è¯­æ³•æ—¶å€™ï¼Œä»ç„¶ä¼šçŠ¯é”™è¯¯ï¼Œæ‰€ä»¥ï¼Œè§£æå™¨è¿˜ä¼šæŠ¥å‘Šè¯­æ³•é”™è¯¯ï¼Œè®©æˆ‘ä»¬çŸ¥é“ã€‚

### 1.3 Static analysis

é™æ€åˆ†æ

The first two stages are pretty similar across all implementations. Now, the individual characteristics of each language start coming into play. At this point, we know the syntactic structure of the codeâ€”things like which expressions are nested in whichâ€”but we donâ€™t know much more than that.

In an expression like a + b, we know we are adding a and b, but we donâ€™t know what those names refer to. Are they local variables? Global? Where are they defined?

å‰é¢ä¸¤ä¸ªé˜¶æ®µï¼ˆæ‰«æã€è§£æï¼‰åœ¨æ‰€æœ‰çš„å®ç°ä¸­éƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œç°åœ¨ï¼Œæ¯ç§è¯­è¨€çš„ç‰¹æ€§å¼€å§‹æ˜¾ç°å‡ºæ¥äº†ã€‚è§£æè¿‡åï¼Œæˆ‘ä»¬çŸ¥é“äº†ä»£ç çš„è¯­æ³•ç»“æ„ï¼Œä¾‹å¦‚ï¼šæˆ‘ä»¬çŸ¥é“äº†åŒ…å«äº†å“ªäº›è¡¨è¾¾å¼ï¼Œä½†æ˜¯æˆ‘ä»¬äº†è§£çš„è¿˜ä¸å¤Ÿå¤šã€‚

åœ¨åƒè¡¨è¾¾å¼ a + bä¸­ï¼Œæˆ‘ä»¬çŸ¥é“è¡¨è¾¾å¼æ˜¯ aä¸bæ±‚å’Œï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸çŸ¥é“aï¼Œbå…·ä½“è¡¨ç¤ºä»€ä¹ˆï¼Œå®ƒä»¬æ˜¯å±€éƒ¨å˜é‡å—ï¼Œæ˜¯å…¨å±€å˜é‡å—ï¼Œå®ƒä»¬æ˜¯åœ¨å“ªé‡Œå®šä¹‰çš„å‘¢ï¼Ÿ

The first bit of analysis that most languages do is called binding or resolution. For each identifier, we find out where that name is defined and wire the two together. This is where scope comes into playâ€”the region of source code where a certain name can be used to refer to a certain declaration.

If the language is statically typed, this is when we type check. Once we know where a and b are declared, we can also figure out their types. Then if those types donâ€™t support being added to each other, we report a type error.

å¤§å¤šæ•°è¯­è¨€çš„ç¬¬ä¸€ç‚¹åˆ†æå«åšï¼Œç»‘å®šæˆ–è§£æã€‚å¯¹äºæ¯ä¸ªæ ‡è¯†ç¬¦ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¯¥æ ‡è¯†ç¬¦åç§°çš„å®šä¹‰ä½ç½®ï¼Œå¹¶ä¸”å°†ä¸¤è€…è¿æ¥åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯ä½œç”¨åŸŸå‘æŒ¥ä½œç”¨çš„åœ°æ–¹â€”â€”æºä»£ç çš„æŸä¸ªåŒºåŸŸä¸­ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªå…·ä½“åç§°æ¥å¼•ç”¨æŸä¸ªå£°æ˜ã€‚

å¦‚æœè¯­è¨€æ˜¯é™æ€è¯­è¨€ï¼Œè¿™æ—¶å€™ï¼Œæˆ‘ä»¬è¿˜è¦è¿›è¡Œç±»å‹åˆ¤æ–­ï¼Œä¸€æ—¦æˆ‘ä»¬æ‰¾åˆ°äº† aï¼Œbçš„å£°æ˜ä½ç½®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è·å–åˆ°å®ƒä»¬çš„æ•°æ®ç±»å‹ã€‚ç„¶åï¼Œå¦‚æœè¿™äº›æ•°æ®ç±»å‹ä¸æ”¯æŒåŠ æ³•è§„åˆ™ï¼Œæˆ‘ä»¬å°†æŠ¥å‘Šä¸€ä¸ªç±»å‹é”™è¯¯ã€‚

> The language weâ€™ll build in this book is dynamically typed, so it will do its type checking later, at runtime.

> æœ¬ä¹¦ä¸­æ„å»ºçš„è¯­è¨€æ˜¯åŠ¨æ€è¯­è¨€ï¼Œæ‰€ä»¥ï¼Œç±»å‹æ£€æŸ¥å°†å‘ç”Ÿåœ¨è¿è¡Œæ—¶ï¼Œè€Œä¸æ˜¯å½“å‰é˜¶æ®µã€‚

Take a deep breath. We have attained the summit of the mountain and a sweeping view of the userâ€™s program. All this semantic insight that is visible to us from analysis needs to be stored somewhere. There are a few places we can squirrel it away:

* Often, it gets stored right back as attributes on the syntax tree itselfâ€”extra fields in the nodes that arenâ€™t initialized during parsing but get filled in later.

* Other times, we may store data in a lookup table off to the side. Typically, the keys to this table are identifiersâ€”names of variables and declarations. In that case, we call it a symbol table and the values it associates with each key tell us what that identifier refers to.

* The most powerful bookkeeping tool is to transform the tree into an entirely new data structure that more directly expresses the semantics of the code. Thatâ€™s the next section.

æ·±å‘¼å¸ï¼Œæˆ‘ä»¬å·²ç»ç™»ä¸Šäº†å±±é¡¶ï¼Œç”¨æˆ·ç¨‹åºä¸€è§ˆæ— ä½™ã€‚ä»åˆ†æé˜¶æ®µåï¼Œè·å–åˆ°çš„è¯­ä¹‰é™„åŠ ä¿¡æ¯ï¼Œéœ€è¦ä¿å­˜åœ¨æŸä¸ªåœ°æ–¹ã€‚æœ‰å‡ ä¸ªåœ°æ–¹å¯ä»¥ä¿å­˜è¿™äº›ä¿¡æ¯ã€‚

* é€šå¸¸ï¼Œå®ƒä½œä¸ºå±æ€§å­˜å‚¨åœ¨è¯­æ³•æ ‘çš„å…¶ä»–å­—æ®µä¸­ï¼Œè¿™äº›å­—æ®µåœ¨è§£æé˜¶æ®µæ²¡æœ‰åˆå§‹åŒ–ï¼Œä½†æ˜¯åœ¨åˆ†æé˜¶æ®µä¼šè¢«å¡«å……

* å…¶ä»–æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•°æ®ä¿å­˜åœ¨æ—è¾¹çš„æŸ¥æ‰¾è¡¨ä¸­ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¯¥æŸ¥æ‰¾è¡¨çš„keyæ˜¯æ ‡è¯†ç¬¦ï¼ˆå˜é‡åç§°å’Œå£°æ˜ï¼‰ã€‚è¿™è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç§°å…¶ä¸ºç¬¦å·è¡¨ï¼Œè¡¨ä¸­keyå¯¹åº”çš„valueï¼Œè¡¨ç¤ºè¯¥æ ‡è¯†ç¬¦å¯¹åº”çš„å®é™…å€¼æ˜¯ä»€ä¹ˆ

* æ›´åŠ å¼ºå¤§çš„è®°å½•æ–¹å¼æ˜¯ï¼Œå°†è¯­æ³•æ ‘è½¬æ¢ä¸ºä¸€ä¸ªå…¨æ–°çš„æ•°æ®ç»“æ„ï¼Œæ›´åŠ ç›´æ¥çš„è¡¨ç¤ºä»£ç çš„å«ä¹‰ã€‚è¿™æ˜¯ä¸‹ä¸€èŠ‚çš„å†…å®¹ã€‚


Everything up to this point is considered the front end of the implementation. You might guess everything after this is the back end, but no. Back in the days of yore when â€œfront endâ€ and â€œback endâ€ were coined, compilers were much simpler. Later researchers invented new phases to stuff between the two halves. Rather than discard the old terms, William Wulf and company lumped those new phases into the charming but spatially paradoxical name middle end.

åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæ‰€æœ‰å†…å®¹é˜¶æ®µï¼Œéƒ½æ˜¯å®ç°çš„å‰ç«¯éƒ¨åˆ†ã€‚ä½ å¯èƒ½ä¼šæƒ³è±¡ï¼Œé‚£ä¹ˆä¹‹åçš„å†…å®¹æ˜¯åç«¯äº†ï¼Œå…¶å®å¹¶ä¸æ˜¯ï¼Œå“ˆå“ˆğŸ˜„ã€‚å½“ç¼–è¯‘å™¨ï¼Œåˆšå¼€å§‹æœ‰å‰ç«¯ã€åç«¯æ¦‚å¿µçš„æ—¶å€™ï¼Œé‚£æ—¶å€™çš„ç¼–è¯‘å™¨éå¸¸ç®€å•ã€‚åæ¥ï¼Œç ”ç©¶äººå‘˜åˆå‘æ˜äº†å…¶ä»–é˜¶æ®µï¼Œå®ƒä»¬åœ¨å‰ç«¯ã€åç«¯ä¹‹é—´ã€‚[William Wulf](https://en.wikipedia.org/wiki/William_Wulf) å’Œä»–ä»¬å…¬å¸å¹¶æ²¡æœ‰æŠ›å¼ƒæ—§çš„æœ¯è¯­ï¼Œä»–ä»¬æŠŠè¿™äº›æ–°å‘æ˜çš„é˜¶æ®µå½’ä¸ºæœ‰æ„æ€çš„ä½†æ˜¯æœ‰äº›çŸ›ç›¾çš„æœ¯è¯­ï¼Œä¸­é—´ç«¯ã€‚

### 1.4 Intermediate representations

ä¸­é—´è¡¨ç¤ºæ³•

You can think of the compiler as a pipeline where each stageâ€™s job is to organize the data representing the userâ€™s code in a way that makes the next stage simpler to implement. The front end of the pipeline is specific to the source language the program is written in. The back end is concerned with the final architecture where the program will run.

In the middle, the code may be stored in some intermediate representation (IR) that isnâ€™t tightly tied to either the source or destination forms (hence â€œintermediateâ€). Instead, the IR acts as an interface between these two languages.

æˆ‘ä»¬å¯ä»¥å°†ç¼–è¯‘å™¨è§†ä¸ºä¸€ä¸ªç®¡é“ï¼Œæ¯ä¸ªé˜¶æ®µçš„å·¥ä½œæ˜¯ç”¨ä¸€ç§æ›´æ˜“äºå®ç°çš„æ–¹å¼ï¼Œç»„ç»‡è¡¨ç¤ºç”¨æˆ·ä»£ç ã€‚ç®¡é“å‰ç«¯ï¼Œä½œç”¨äºç¼–å†™ç¨‹åºçš„æºè¯­è¨€ï¼Œåç«¯ä¸ç¨‹åºè¿è¡Œçš„æœ€ç»ˆæ¶æ„ç›¸å…³ã€‚

åœ¨ä¸­é—´ç«¯ï¼Œä»£ç å¯èƒ½å­˜å‚¨åœ¨ä¸€äº›ä¸­é—´è¡¨ç¤ºä¸­ï¼Œè¿™äº›è¡¨ç¤ºï¼Œä¸æºè¯­è¨€å’Œç›®æ ‡å½¢å¼éƒ½æ²¡æœ‰ç´§å¯†å…³è”ã€‚ç›¸åï¼Œè¿™ç§ä¸­é—´è¡¨ç¤ºï¼Œå……å½“æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä¹‹é—´çš„æ¥å£ã€‚

> There are a few well-established styles of IRs out there. Hit your search engine of choice and look for â€œcontrol flow graphâ€, â€œstatic single-assignmentâ€, â€œcontinuation-passing styleâ€, and â€œthree-address codeâ€.

> æœ‰ä¸€äº›æˆç†Ÿçš„ä¸­é—´è¡¨ç¤ºé£æ ¼ï¼Œæœ‰å…´è¶£çš„è¯ï¼Œå¯ä»¥å»ç ”ç©¶ä¸€ä¸‹ï¼Œæ§åˆ¶æµå›¾ã€é™æ€å•èµ‹å€¼ã€è¿ç»­ä¼ é€’æ ·å¼ã€ä¸‰åœ°å€ä»£ç ç­‰ç­‰ã€‚

This lets you support multiple source languages and target platforms with less effort. Say you want to implement Pascal, C, and Fortran compilers, and you want to target x86, ARM, and, I dunno, SPARC. Normally, that means youâ€™re signing up to write nine full compilers: Pascalâ†’x86, Câ†’ARM, and every other combination.

A shared intermediate representation reduces that dramatically. You write one front end for each source language that produces the IR. Then one back end for each target architecture. Now you can mix and match those to get every combination.

Thereâ€™s another big reason we might want to transform the code into a form that makes the semantics more apparentâ€‰.â€‰.â€‰. 

è¿™ç§æ–¹å¼ï¼Œå¯ä»¥è®©æˆ‘ä»¬æ›´å®¹æ˜“æ”¯æŒå¤šç§æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„åŒ¹é…ã€‚å‡è®¾ä½ æƒ³è¦å®ç°ï¼ŒPascalã€Cã€Fortranè¯­è¨€çš„ç¼–è¯‘å™¨ï¼Œå¹¶ä¸”è¿™äº›ç¼–è¯‘å™¨ï¼Œå¯ä»¥è¿è¡Œåœ¨X86ï¼Œ armï¼ŒSPARCç­‰æ¶æ„ä¸Šï¼Œå¦‚æœæ²¡æœ‰ä½¿ç”¨ä¸­é—´è¡¨ç¤ºï¼Œä½ éœ€è¦å®ç°9ç§ç¼–è¯‘å™¨ï¼Œä¾‹å¦‚: Pascalâ€”â€”> x86ï¼ŒCâ€”â€”> armç­‰ç­‰ã€‚

ä½¿ç”¨ä¸€ç§å…±äº«çš„ä¸­é—´è¡¨ç¤ºï¼Œå¯ä»¥å¤§å¤§å‡å°‘è¿™äº›ç»„åˆã€‚å¯¹äºæ¯ä¸€ç§æºè¯­è¨€ï¼Œç¼–å†™ä¸€ä¸ªå‰ç«¯ï¼Œå°†æºè¯­è¨€è§£æä¸ºä¸­é—´è¡¨ç¤ºï¼Œå¯¹äºæ¯ä¸ªç›®æ ‡æ¶æ„ï¼Œé’ˆå¯¹ä¸­é—´è¡¨ç¤ºï¼Œç¼–å†™ä¸€ä¸ªåç«¯ã€‚æ‰€ä»¥ï¼Œç°åœ¨åªéœ€è¦å®ç° 6 ä¸­å‰ç«¯ã€åç«¯ç»„åˆã€‚

è¿˜æœ‰ä¸€ä¸ªé‡è¦åŸå› ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä¸­é—´è¡¨ç¤ºã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸­é—´è¡¨ç¤ºï¼Œå°†ä»£ç è½¬ä¸ºä¸€ç§å½¢å¼ï¼Œä½¿å¾—è¯­ä¹‰æ›´åŠ æ˜æ˜¾ã€‚

>If youâ€™ve ever wondered how GCC supports so many crazy languages and architectures, like Modula-3 on Motorola 68k, now you know. Language front ends target one of a handful of IRs, mainly GIMPLE and RTL. Target back ends like the one for 68k then take those IRs and produce native code.
>
> å¦‚æœä½ æƒ³è¦çŸ¥é“ [GCC](https://en.wikipedia.org/wiki/GNU_Compiler_Collection)æ˜¯å¦‚ä½•æ”¯æŒè¿™ä¹ˆå¤šè¯­è¨€å’Œæ¶æ„çš„ï¼Œç°åœ¨ä½ çŸ¥é“åŸå› äº†ã€‚é’ˆå¯¹å‰ç«¯çš„å°‘é‡ä¸­é—´è¡¨ç¤ºï¼Œä¸»è¦æ˜¯ [GIMPLE](https://gcc.gnu.org/onlinedocs/gccint/GIMPLE.html) å’Œ [RTL](https://gcc.gnu.org/onlinedocs/gccint/RTL.html), ä¸åŒçš„ç›®æ ‡æ¶æ„ï¼Œè·å–ä¸­é—´è¡¨ç¤ºï¼Œç”Ÿæˆå¯¹åº”çš„æœºå™¨ä»£ç ã€‚

### 1.5 Optimization

ä¼˜åŒ–

Once we understand what the userâ€™s program means, we are free to swap it out with a different program that has the same semantics but implements them more efficientlyâ€”we can optimize it.

A simple example is constant folding: if some expression always evaluates to the exact same value, we can do the evaluation at compile time and replace the code for the expression with its result. If the user typed in this:

```

pennyArea = 3.14159 * (0.75 / 2) * (0.75 / 2);

```

we could do all of that arithmetic in the compiler and change the code to:

```
pennyArea = 0.4417860938;
```

ä¸€æ—¦æˆ‘ä»¬ç†è§£äº†ç”¨æˆ·ç¨‹åºçš„å«ä¹‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥è‡ªç”±çš„æŠŠç”¨æˆ·ç¨‹åºæ›¿æ¢ä¸ºå…·æœ‰ç›¸åŒè¯­ä¹‰ï¼Œä½†æ˜¯æ›´åŠ é«˜æ•ˆçš„å®ç°çš„å¦å¤–ä¸€ç§ç¨‹åºã€‚æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¼˜åŒ–ã€‚

ä¸€ä¸ªç®€å•ç¤ºä¾‹æ˜¯å¸¸æ•°è®¡ç®—ã€‚å¦‚æœæŸä¸ªè¡¨è¾¾å¼çš„è®¡ç®—ç»“æœæ€»æ˜¯ç›¸åŒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨ç¼–è¯‘æ—¶å€™è®¡ç®—è¯¥è¡¨è¾¾å¼ï¼Œå¹¶ä¸”ä½¿ç”¨è¡¨è¾¾å¼è®¡ç®—ç»“æœä»£æ›¿è¯¥è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼Œå®¢æˆ·è¾“å…¥

```
pennyArea = 3.14159 * (0.75 / 2) * (0.75 / 2);
```

æˆ‘ä»¬å¯ä»¥åœ¨ç¼–è¯‘æ—¶å€™è®¡ç®—å‡ºå€¼ï¼Œå°†ä»£ç ä¿®æ”¹ä¸º

```
pennyArea = 0.4417860938;
```

Optimization is a huge part of the programming language business.  Many language hackers spend their entire careers here, squeezing every drop of performance they can out of their compilers to get their benchmarks a fraction of a percent faster. It can become a sort of obsession.

Weâ€™re mostly going to hop over that rathole in this book. Many successful languages have surprisingly few compile-time optimizations. For example, Lua and CPython generate relatively unoptimized code, and focus most of their performance effort on the runtime.

ä¼˜åŒ–æ˜¯ç¼–ç¨‹è¯­è¨€çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼Œè®¸å¤šè¯­è¨€é«˜æ‰‹åœ¨æ•´ä¸ªèŒä¸šç”Ÿæ¶¯éƒ½åœ¨ä¸æ–­ä¼˜åŒ–ï¼Œä»ç¼–è¯‘å™¨ä¸­æ¦¨å–æ¯ä¸€ç‚¹æ€§èƒ½ï¼Œæœ€ç»ˆä½¿å¾—ä»–ä»¬çš„ç¼–è¯‘å™¨ï¼ŒåŸºå‡†æµ‹è¯•ç»“æœæé«˜äº†0.5%ï¼Œä¼˜åŒ–æ˜¯ä¸€ä¸ªä¸æ–­è¿›è¡Œçš„è¿‡ç¨‹ã€‚

æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬ä¼šè·³è¿‡ä¼˜åŒ–è¿™ä¸ªæ­¥éª¤ï¼Œæœ‰å¾ˆå¤šæˆåŠŸè¯­è¨€ï¼Œä¹Ÿå¾ˆå°‘ä½¿ç”¨ç¼–è¯‘æ—¶ä¼˜åŒ–ã€‚ä¸¾ä¾‹ï¼ŒLua å’Œ CPythonç”Ÿæˆç›¸å¯¹æœªä¼˜åŒ–çš„ä»£ç ï¼Œå°†å¤§éƒ¨åˆ†ä¼˜åŒ–æ”¾åœ¨è¿è¡Œæ—¶ã€‚

> If you canâ€™t resist poking your foot into that hole, some keywords to get you started are â€œconstant propagationâ€, â€œcommon subexpression eliminationâ€, â€œloop invariant code motionâ€, â€œglobal value numberingâ€, â€œstrength reductionâ€, â€œscalar replacement of aggregatesâ€, â€œdead code eliminationâ€, and â€œloop unrollingâ€.
>
> å¦‚æœä½ å¿ä¸ä½æƒ³è¦æ¢ç´¢ä¼˜åŒ–è¿™ä¸ªé¢†åŸŸï¼Œé‚£ä¹ˆä½ å¯ä»¥ä»ä¸€äº›æœ¯è¯­å…¥æ‰‹ï¼Œä¾‹å¦‚: "æ’å®šä¼ æ’­", "å…¬å…±å­è¡¨è¾¾å¼æ¶ˆé™¤", "å¾ªç¯ä¸å˜ä»£ç ", "å…¨å±€å€¼ç¼–å·", "å¼ºåº¦é™ä½", "èšåˆæ ‡é‡ä»£æ›¿", "åƒµå°¸ä»£ç æ¶ˆé™¤", "å¾ªç¯å±•å¼€"ã€‚

### 1.6 Code generation

ä»£ç ç”Ÿæˆ

We have applied all of the optimizations we can think of to the userâ€™s program. The last step is converting it to a form the machine can actually run. In other words, generating code (or code gen), where â€œcodeâ€ here usually refers to the kind of primitive assembly-like instructions a CPU runs and not the kind of â€œsource codeâ€ a human might want to read.

Finally, we are in the back end, descending the other side of the mountain. From here on out, our representation of the code becomes more and more primitive, like evolution run in reverse, as we get closer to something our simple-minded machine can understand.

We have a decision to make. Do we generate instructions for a real CPU or a virtual one? If we generate real machine code, we get an executable that the OS can load directly onto the chip. Native code is lightning fast, but generating it is a lot of work. Native code is lightning fast, but generating it is a lot of work. 

æˆ‘ä»¬å·²ç»å°†èƒ½å¤Ÿæƒ³åˆ°çš„æ‰€æœ‰ä¼˜åŒ–ï¼Œåº”ç”¨äºç”¨æˆ·ä»£ç ä¸­ã€‚æœ€åä¸€æ­¥æ˜¯ï¼Œå°†ä»£ç è½¬æ¢ä¸ºæœºå™¨å¯ä»¥å®é™…è¿è¡Œçš„å½¢å¼ã€‚æ¢å¥è¯è¯´ï¼Œä»£ç ç”Ÿæˆï¼ˆæˆ–è€…ç”Ÿæˆä»£ç ï¼‰ï¼Œè¿™é‡Œçš„ä»£ç æ˜¯æŒ‡ï¼ŒCPUç›´æ¥è¿è¡Œçš„ç±»ä¼¼äºåŸå§‹æ±‡ç¼–çš„æŒ‡ä»¤ï¼Œè€Œä¸ç”¨äººä»¬å¯ä»¥ç›´æ¥é˜…è¯»çš„æºä»£ç ã€‚

æœ€åï¼Œæˆ‘ä»¬å¤„äºåç«¯ï¼Œä»å±±çš„å¦ä¸€ç«¯å¾€ä¸‹èµ°ã€‚ä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä»¬å¯¹äºä»£ç çš„è¡¨ç¤ºè¶Šæ¥è¶ŠåŸå§‹ï¼Œå’Œä¸Šå±±æ–¹å‘ç›¸åï¼Œæˆ‘ä»¬å¸Œæœ›ä»£ç å˜ä¸ºæœºå™¨å¯ä»¥ç›´æ¥è¿è¡Œçš„å½¢å¼ã€‚

æˆ‘ä»¬éœ€è¦å†³å®šï¼Œæ˜¯ç”ŸæˆçœŸå®çš„CPUæŒ‡ä»¤ï¼Œè¿˜æ˜¯ç”Ÿæˆè™šæ‹Ÿçš„æŒ‡ä»¤ã€‚å¦‚æœæˆ‘ä»¬ç”ŸæˆçœŸå®çš„æœºå™¨ä»£ç ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ï¼Œæ“ä½œç³»ç»Ÿå¯ä»¥ç›´æ¥åŠ è½½åˆ°èŠ¯ç‰‡ä¸­ã€‚åŸç”Ÿæœ¬æœºä»£ç è¿è¡Œé€Ÿåº¦éå¸¸å¿«ï¼Œä½†æ˜¯ï¼Œç”ŸæˆçœŸå®çš„ CPUæŒ‡ä»¤éœ€è¦å¤§é‡çš„å·¥ä½œã€‚ç°åœ¨çš„æ¶æ„ï¼Œæœ‰æˆå †çš„æŒ‡ä»¤é›†ï¼Œå¤æ‚çš„ç®¡é“ï¼Œå’Œèƒ½å¤Ÿå¡æ»¡747é£æœºçš„å†å²é—ç•™åŒ…è¢±ã€‚

> For example, the AAD (â€œASCII Adjust AX Before Divisionâ€) instruction lets you perform division, which sounds useful. Except that instruction takes, as operands, two binary-coded decimal digits packed into a single 16-bit register. When was the last time you needed BCD on a 16-bit machine
>
> ä¸¾ä¾‹ï¼ŒAADæŒ‡ä»¤å¯ä»¥æ‰§è¡Œé™¤æ³•è¿ç®—ï¼Œè¿™å¬èµ·æ¥éå¸¸æœ‰ç”¨ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒADDæŒ‡ä»¤ä¼šå°†ä¸¤ä¸ªäºŒè¿›åˆ¶ç¼–ç é¢åè¿›åˆ¶æ•°å­—ä½œä¸ºæ“ä½œæ•°å‹ç¼©åˆ°å•ä¸ª16ä½å¯„å­˜å™¨ä¸Šï¼Œä¸Šä¸€æ¬¡ï¼Œä½ éœ€è¦BCDï¼Œå¹¶ä¸”åœ¨16ä½æœºå™¨ï¼Œæ˜¯ä»€ä¹ˆæ—¶å€™å‘¢ï¼Ÿ

Speaking the chipâ€™s language also means your compiler is tied to a specific architecture. If your compiler targets x86 machine code, itâ€™s not going to run on an ARM device.  All the way back in the â€™60s, during the Cambrian explosion of computer architectures, that lack of portability was a real obstacle.

To get around that, hackers like Martin Richards and Niklaus Wirth, of BCPL and Pascal fame, respectively, made their compilers produce virtual machine code. Instead of instructions for some real chip, they produced code for a hypothetical, idealized machine. Wirth called this p-code for portable, but today, we generally call it bytecode because each instruction is often a single byte long.

These synthetic instructions are designed to map a little more closely to the languageâ€™s semantics, and not be so tied to the peculiarities of any one computer architecture and its accumulated historical cruft. You can think of it like a dense, binary encoding of the languageâ€™s low-level operations.

è½¬æ¢ä¸ºèŠ¯ç‰‡ç‰¹å®šè¯­è¨€ï¼Œæ„å‘³ç€ä½ çš„ç¼–è¯‘å™¨å’Œç‰¹å®šæ¶æ„ç›¸å…³è”ã€‚å¦‚æœç¼–è¯‘å™¨ä»¥x86 æœºå™¨ä»£ç ä¸ºç›®æ ‡ï¼Œé‚£ä¹ˆå®ƒå°†æ— æ³•åœ¨armæ¶æ„æœºå™¨ä¸Šè¿è¡Œã€‚ä¸€ç›´è¿½æº¯åˆ°ä¸Šä¸–çºª60å¹´ä»£ï¼Œåœ¨å½“æ—¶çš„è®¡ç®—æœºä½“ç³»ç»“æ„çˆ†ç‚¸æ—¶æœŸï¼Œç¼ºä¹å¯ç§»æ¤æ€§çš„ç¼–è¯‘å™¨æ˜¯ä¸€ä¸ªçœŸæ­£çš„ç¼ºç‚¹ã€‚

ä¸ºäº†é¿å…è¿™ç§é—®é¢˜ï¼Œç¼–ç¨‹é«˜æ‰‹ï¼Œä¾‹å¦‚ï¼šBCPL è¯­è¨€çš„å‘æ˜è€…[Martin Richards](https://en.wikipedia.org/wiki/Martin_Richards_(computer_scientist)) å’Œ Pascalè¯­è¨€çš„ä¸»è¦å¼€å‘è€…[Niklaus Wirth](https://en.wikipedia.org/wiki/Niklaus_Wirth) ,ä¸çº¦è€ŒåŒçš„ï¼Œè®©ä»–ä»¬å®ç°çš„ç¼–è¯‘å™¨æœ€ç»ˆç”Ÿæˆè™šæ‹Ÿæœºä»£ç ã€‚ä»–ä»¬ä¸ºä¸€ä¸ªå‡æƒ³çš„ã€ç†æƒ³åŒ–çš„æœºå™¨ç”Ÿæˆä»£ç ï¼Œè€Œä¸æ˜¯ä¸ºäº†ä¸€äº›çœŸå®çš„èŠ¯ç‰‡ç”ŸæˆæŒ‡ä»¤ã€‚[Niklaus Wirth](https://en.wikipedia.org/wiki/Niklaus_Wirth)ç§°è¿™äº›ä»£ç ä¸ºPä»£ç ï¼Œå› ä¸ºå¯ç§»æ¤å•è¯çš„ç¼©å†™ï¼Œä½†æ˜¯ä»Šå¤©ï¼Œæˆ‘ä»¬é€šå¸¸ç§°ä¸ºå­—èŠ‚ç ï¼Œå› ä¸ºæ¯æ¡æŒ‡ä»¤é€šå¸¸åªæœ‰ä¸€ä¸ªå­—èŠ‚é•¿åº¦ã€‚

è¿™äº›åˆæˆæŒ‡ä»¤ï¼Œæ˜¯ä¸ºäº†æ›´åŠ æ¥è¿‘ä»£ç çš„è¯­ä¹‰ï¼Œè€Œä¸æ˜¯å› ä¸ºæ›´åŠ å…³è”é‚£äº›æ¶æ„æˆ–æ˜¯å…¶åçš„å†å²ã€‚æˆ‘ä»¬å¯ä»¥æƒ³è±¡å­—èŠ‚ç æ˜¯æ›´åŠ åº•å±‚çš„äºŒè¿›åˆ¶ç¼–ç ã€‚

> The basic principle here is that the farther down the pipeline you push the architecture-specific work, the more of the earlier phases you can share across architectures.
> 
> åŸºæœ¬çš„åŸåˆ™æ˜¯ï¼Œè¶Šæ™šæŠŠç¼–è¯‘å™¨å±€é™äºç‰¹å®šæ¶æ„ä¸Šï¼Œå°±å¯ä»¥è¶Šå¤šçš„äº«å—ä¸åŒæ¶æ„é—´ä»£ç å…±ç”¨ã€‚

>There is a tension, though. Many optimizations, like register allocation and instruction selection, work best when they know the strengths and capabilities of a specific chip. Figuring out which parts of your compiler can be shared and which should be target-specific is an art.
>
> ä¸è¿‡ï¼Œå­˜åœ¨ä¸€ç§å‡è¡¡ï¼Œè®¸å¤šä¼˜åŒ–ï¼Œä¾‹å¦‚ï¼šå¯„å­˜å™¨åˆ†é…å’ŒæŒ‡ä»¤é€‰æ‹©ï¼Œå½“ä½ äº†è§£ç‰¹å®šèŠ¯ç‰‡çš„ä¼˜ç‚¹å’Œèƒ½åŠ›æ—¶å€™ï¼Œä½¿ç”¨æ•ˆæœä¼šæ›´å¥½ã€‚ææ¸…æ¥šç¼–è¯‘å™¨å“ªäº›éƒ¨åˆ†å¯ä»¥å…±äº«ï¼Œå“ªäº›éƒ¨åˆ†åªé€‚ç”¨äºç‰¹å®šæ¶æ„ï¼Œæ˜¯ä¸€é—¨è‰ºæœ¯ã€‚


### 1.7 Virtual machine

è™šæ‹Ÿæœº

If your compiler produces bytecode, your work isnâ€™t over once thatâ€™s done. Since there is no chip that speaks that bytecode, itâ€™s your job to translate. Again, you have two options. You can write a little mini-compiler for each target architecture that converts the bytecode to native code for that machine. You still have to do work for each chip you support, but this last stage is pretty simple and you get to reuse the rest of the compiler pipeline across all of the machines you support. Youâ€™re basically using your bytecode as an intermediate representation.

Or you can write a virtual machine (VM), a program that emulates a hypothetical chip supporting your virtual architecture at runtime. Running bytecode in a VM is slower than translating it to native code ahead of time because every instruction must be simulated at runtime each time it executes. In return, you get simplicity and portability. Implement your VM in, say, C, and you can run your language on any platform that has a C compiler. This is how the second interpreter we build in this book works.

å¦‚æœä½ çš„ç¼–è¯‘å™¨ï¼Œæœ€ç»ˆäº§ç”Ÿå­—èŠ‚ç ï¼Œé‚£ä¹ˆï¼Œä½ çš„å·¥ä½œè¿˜æ²¡æœ‰ç»“æŸã€‚å¦‚æœä½ çš„å·¥ä½œæ˜¯ç¿»è¯‘çš„è¯ï¼Œé‚£ä¹ˆç°åœ¨æ²¡æœ‰èŠ¯ç‰‡å¯ä»¥ç›´æ¥æ‰§è¡Œå­—èŠ‚ç ã€‚åŒæ ·ï¼Œç°åœ¨ä½ æœ‰ä¸¤ä¸ªé€‰æ‹©ã€‚

ç¬¬ä¸€ä¸ªé€‰æ‹©æ˜¯ï¼Œä½ å¯ä»¥ä¸ºæ¯ä¸€ç§ç›®æ ‡æ¶æ„ç¼–å†™ä¸€ä¸ªå°å‹ç¼–è¯‘å™¨ï¼ŒæŠŠå­—èŠ‚ç è½¬æ¢ä¸ºæ¶æ„ä¸­æœºå™¨ä½¿ç”¨çš„æœºå™¨ç ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºæ”¯æŒçš„æ¯ä¸€ç§èŠ¯ç‰‡ç¼–å†™å¯¹åº”çš„ç¼–è¯‘å™¨ï¼Œä½†æ˜¯æœ€åè¿™ä¸ªé˜¶æ®µéå¸¸ç®€å•ï¼Œä½ ä¹Ÿå¯ä»¥é‡å¤ä½¿ç”¨ç¼–è¯‘å™¨ä¹‹å‰çš„ä»£ç ã€‚æˆ‘ä»¬ä½¿ç”¨ç¼–è¯‘å™¨ç”Ÿæˆçš„å­—èŠ‚ç å½“ä½œä¸­é—´è¡¨ç¤ºã€‚

æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªè™šæ‹Ÿæœºï¼Œä¸€ä¸ªåœ¨è¿è¡Œæ—¶å€™ï¼Œæ¨¡æ‹Ÿè™šæ„èŠ¯ç‰‡çš„ç¨‹åºã€‚åœ¨è™šæ‹Ÿæœºä¸­è¿è¡Œå­—èŠ‚ç æ¯”ç›´æ¥åœ¨æœ¬æœºè¿è¡Œæœºå™¨ç ï¼Œæ…¢ä¸€äº›ï¼Œå› ä¸ºè™šæ‹Ÿæœºæ¯æ¬¡è¿è¡ŒæŒ‡ä»¤ï¼Œéƒ½å¿…é¡»æ¨¡æ‹Ÿå®é™…æŒ‡ä»¤ã€‚ä½œä¸ºå›æŠ¥ï¼Œæˆ‘ä»¬è·å¾—äº†ç®€å•æ€§å’Œå¯ç§»æ¤æ€§ã€‚å‡è®¾ç”¨Cè¯­è¨€å®ç°è™šæ‹Ÿæœºï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æœ‰Cç¼–è¯‘å™¨çš„æœºå™¨ä¸Šè¿è¡Œä»£ç ã€‚æœ¬ä¹¦ç¬¬äºŒéƒ¨åˆ†å®ç°çš„ç¼–è¯‘å™¨é‡‡ç”¨è¯¥åŸç†ã€‚

> The term â€œvirtual machineâ€ also refers to a different kind of abstraction. A system virtual machine emulates an entire hardware platform and operating system in software. This is how you can play Windows games on your Linux machine, and how cloud providers give customers the user experience of controlling their own â€œserverâ€ without needing to physically allocate separate computers for each user.
>
>The kind of VMs weâ€™ll talk about in this book are language virtual machines or process virtual machines if you want to be unambiguous.
>
> æœ¯è¯­è™šæ‹Ÿæœºæ˜¯ä¸€ç§æŠ½è±¡ã€‚ä¸€ä¸ªç³»ç»Ÿçº§åˆ«çš„è™šæ‹Ÿæœºï¼Œä¼šæ¨¡æ‹Ÿæ•´ä¸ªç¡¬ä»¶å¹³å°å’Œæ“ä½œç³»ç»Ÿã€‚è¿™å°±æ˜¯ï¼Œä½ å¯ä»¥åœ¨LinuxæœåŠ¡å™¨ä¸Šç©Windowsæ¸¸æˆçš„åŸå› ï¼Œè¿˜æœ‰ï¼Œè¿™ä¹Ÿæ˜¯äº‘æœåŠ¡å‚å•†ï¼Œä¸ºç”¨æˆ·åˆ†é…æŒ‡å®šçš„äº‘æœåŠ¡å™¨ï¼Œè€Œä¸éœ€è¦çœŸå®æä¾›å¯¹åº”çš„æœºå™¨çš„åŸå› ã€‚
>
> ä½†æ˜¯ï¼Œæœ¬ä¹¦ä¸­æ¶‰åŠåˆ°çš„è™šæ‹Ÿæœºï¼Œåªæ˜¯è¯­è¨€è™šæ‹Ÿæœºæˆ–è€…è¿›ç¨‹è™šæ‹Ÿæœºï¼Œå¦‚æœä½ æƒ³è¦ä¸€ä¸ªå‡†ç¡®çš„æè¿°è¯ã€‚

### 1.8 Runtime

è¿è¡Œæ—¶

We have finally hammered the userâ€™s program into a form that we can execute. The last step is running it. If we compiled it to machine code, we simply tell the operating system to load the executable and off it goes. If we compiled it to bytecode, we need to start up the VM and load the program into that.

In both cases, for all but the basest of low-level languages, we usually need some services that our language provides while the program is running. For example, if the language automatically manages memory, we need a garbage collector going in order to reclaim unused bits. If our language supports â€œinstance ofâ€ tests so you can see what kind of object you have, then we need some representation to keep track of the type of each object during execution.

All of this stuff is going at runtime, so itâ€™s called, appropriately, the runtime. In a fully compiled language, the code implementing the runtime gets inserted directly into the resulting executable.  In, say, Go, each compiled application has its own copy of Goâ€™s runtime directly embedded in it. If the language is run inside an interpreter or VM, then the runtime lives there. This is how most implementations of languages like Java, Python, and JavaScript work.

æœ€ç»ˆï¼Œæˆ‘ä»¬ç»ˆäºæŠŠç”¨æˆ·ç¨‹åºï¼Œè½¬æ¢ä¸ºä¸€ç§å¯ä»¥æ‰§è¡Œçš„å½¢å¼ã€‚æœ€åä¸€æ­¥æ˜¯è¿è¡Œï¼Œå¦‚æœæˆ‘ä»¬æœ€åç¼–è¯‘ä¸ºæœºå™¨ç ï¼Œæˆ‘ä»¬åªéœ€è¦åŠ è½½å¯æ‰§è¡Œæ–‡ä»¶ï¼Œç„¶åè¿è¡Œã€‚å¦‚æœå°†å…¶ç¼–è¯‘ä¸ºå­—èŠ‚ç ï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨è™šæ‹Ÿæœºï¼ŒæŠŠç¼–è¯‘çš„å­—èŠ‚ç åŠ è½½åˆ°è™šæ‹Ÿæœºä¸­ã€‚

åœ¨ä¸¤ç§åœºæ™¯ä¸‹ï¼Œé™¤äº†æœ€åŸºæœ¬çš„ä½å±‚è¯­è¨€ï¼Œå½“ç¨‹åºè¿è¡Œæ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æä¾›ä¸€äº›å…¶ä»–æœåŠ¡ã€‚ä¾‹å¦‚ï¼šå¦‚æœè¯­è¨€æ˜¯è‡ªåŠ¨ç®¡ç†å†…å­˜çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåƒåœ¾æ”¶é›†å™¨ï¼Œå›æ”¶ä¸å†ä½¿ç”¨çš„å†…å­˜ã€‚å¦‚æœæˆ‘ä»¬å®ç°çš„è¯­è¨€ï¼Œæ”¯æŒå®ä¾‹æµ‹è¯•ï¼Œä»¥ä¾¿äºè·å–å¯¹è±¡çš„å®é™…æ•°æ®ç±»å‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€äº›åŠŸèƒ½ï¼Œè·Ÿè¸ªè¿è¡Œæ—¶å€™çš„å¯¹è±¡ã€‚

æ‰€æœ‰è¿™äº›éƒ½å‘ç”Ÿåœ¨ç¨‹åºè¿è¡Œæ—¶å€™ï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬ç§°å‘¼è¿™ä¸ªé˜¶æ®µä¸ºè¿è¡Œæ—¶ã€‚åœ¨ä¸€ä¸ªå®Œå…¨ç¼–è¯‘çš„è¯­è¨€ä¸­ï¼Œè¿è¡Œæ—¶ï¼Œä»£ç å°†ç›´æ¥æ’å…¥åˆ°å¯æ‰§è¡Œæ–‡ä»¶ä¸­ã€‚ä¸¾ä¾‹ï¼Œåœ¨goè¯­è¨€ä¸­ï¼Œæ¯ä¸ªç¼–è¯‘çš„åº”ç”¨ç¨‹åºéƒ½æœ‰è‡ªå·±çš„ goè¿è¡Œæ—¶å‰¯æœ¬ï¼Œç›´æ¥åµŒå…¥å…¶ä¸­ã€‚å¦‚æœè¯­è¨€åœ¨è§£é‡Šå™¨æˆ–è€…è™šæ‹Ÿæœºä¸­è¿è¡Œï¼Œé‚£ä¹ˆè¿è¡Œæ—¶å°±åœ¨å…¶ä¸­ã€‚è¿™ä¹Ÿæ˜¯ Java/Python/JavaScriptç­‰è¯­è¨€çš„è¿è¡Œå·¥ä½œæ–¹å¼ã€‚

## äºŒã€Shortcuts and Alternate Routes

å¿«æ·æ–¹å¼å’Œå¤‡é€‰è·¯å¾„

Thatâ€™s the long path covering every possible phase you might implement. Many languages do walk the entire route, but there are a few shortcuts and alternate paths.

è¿™ä¸€æ¡æ¼«é•¿çš„è·¯ï¼Œå¯èƒ½åŒ…å«ä½ çš„å®ç°çš„æ¯ä¸€ä¸ªé˜¶æ®µã€‚è®¸å¤šè¯­è¨€è´¯ç©¿äº†æ•´ä¸ªè¿‡ç¨‹ï¼Œä½†æ˜¯ä¹Ÿæœ‰ä¸€äº›è¯­è¨€ï¼Œä¼šåŒ…å«æ·å¾„å’Œå…¶ä»–å¤‡é€‰è·¯å¾„ã€‚

### 2.1 Single-pass compilers

å•é€šé“ç¼–è¯‘å™¨

Some simple compilers interleave parsing, analysis, and code generation so that they produce output code directly in the parser, without ever allocating any syntax trees or other IRs. These single-pass compilers restrict the design of the language. You have no intermediate data structures to store global information about the program, and you donâ€™t revisit any previously parsed part of the code. That means as soon as you see some expression, you need to know enough to correctly compile it.

Pascal and C were designed around this limitation. At the time, memory was so precious that a compiler might not even be able to hold an entire source file in memory, much less the whole program. This is why Pascalâ€™s grammar requires type declarations to appear first in a block. Itâ€™s why in C you canâ€™t call a function above the code that defines it unless you have an explicit forward declaration that tells the compiler what it needs to know to generate code for a call to the later function.

ä¸€äº›ç®€å•çš„ç¼–è¯‘å™¨ï¼Œä¼šæŠŠè§£æé˜¶æ®µã€åˆ†æé˜¶æ®µã€ä»£ç ç”Ÿæˆé˜¶æ®µæ··æ‚åœ¨ä¸€èµ·ï¼Œå®ƒä»¬ç›´æ¥åœ¨è§£æè¿‡ç¨‹ç”Ÿæˆä»£ç ï¼Œè€Œä¸éœ€è¦ç”Ÿæˆè¯­æ³•æ ‘æˆ–è€…å…¶ä»–ä¸­é—´è¡¨ç¤ºã€‚è¿™äº›å•é€šé“ç¼–è¯‘å™¨é™åˆ¶äº†è¯­è¨€çš„è®¾è®¡ã€‚ä½ æ²¡æœ‰ä¸­é—´æ•°æ®ç»“æ„æ¥å­˜å‚¨æœ‰å…³ç¨‹åºçš„å…¨å±€ä¿¡æ¯ï¼Œä¹Ÿæ— æ³•é‡æ–°è®¿é—®ä»»ä½•ä¹‹å‰è§£æè¿‡çš„ä»£ç ã€‚è¿™æ„å‘³ç€ï¼Œä¸€æ—¦çœ‹åˆ°æŸä¸ªè¡¨è¾¾å¼ï¼Œæˆ‘ä»¬éœ€è¦è¶³å¤Ÿçš„ä¿¡æ¯æ¥ï¼Œæ­£ç¡®çš„ç¼–è¯‘è¡¨è¾¾å¼ã€‚

Pascal å’Œ Cè¯­è¨€æ˜¯å›´ç»•ä¸Šé¢çš„é™åˆ¶è®¾è®¡çš„ã€‚å½“æ—¶ï¼Œå†…å­˜éå¸¸å®è´µï¼Œç¼–è¯‘å™¨ç”šè‡³æ— æ³•å°†æ•´ä¸ªæºæ–‡ä»¶æ”¾å…¥åˆ°å†…å­˜ä¸­ï¼Œæ›´ä¸ç”¨è¯´åœ¨å†…å­˜ä¸­ï¼Œä¿å­˜æ•´ä¸ªç¨‹åºäº†ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆPascalè¯­è¨€è¦æ±‚ç±»å‹å£°æ˜é¦–å…ˆå‡ºç°åœ¨ä»£ç å—ä¸­ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨Cè¯­è¨€ä¸­ï¼Œé™¤éæœ‰ä¸€ä¸ªæ˜ç¡®çš„æ­£å‘å£°æ˜ï¼Œå‘Šè¯‰ç¼–è¯‘å™¨ç”Ÿæˆè°ƒç”¨åé¢çš„å‡½æ•°æ‰€éœ€è¦çš„ä»£ç ï¼Œå¦åˆ™æ— æ³•åœ¨å‡½æ•°å®šä¹‰çš„ä½ç½®ä¸Šé¢ï¼Œè°ƒç”¨è¯¥å‡½æ•°ã€‚

> Syntax-directed translation is a structured technique for building these all-at-once compilers. You associate an action with each piece of the grammar, usually one that generates output code. Then, whenever the parser matches that chunk of syntax, it executes the action, building up the target code one rule at a time.
> 
> è¯­æ³•å®šå‘ç¿»è¯‘æ˜¯ä¸€ç§ç»“æ„åŒ–æŠ€æœ¯ï¼Œç”¨äºåŒæ—¶æ„å»ºè¿™äº›ç¼–è¯‘å™¨ï¼Œå°†æ¯ä¸€ä¸ªåŠ¨ä½œå’Œæ¯ä¸€ä¸ªè¯­æ³•ç‰‡æ®µç›¸å…³è”ï¼Œé€šå¸¸æ˜¯è¾“å‡ºä»£ç çš„è¯­æ³•ç‰‡æ®µã€‚ç„¶åï¼Œæ¯å½“è§£æå™¨åŒ¹é…è¯¥è¯­æ³•å—æ—¶å€™ï¼Œå®ƒä¼šæ‰§è¡Œæ“ä½œï¼Œä¸€æ¬¡å»ºç«‹ä¸€ä¸ªè§„åˆ™çš„ç›®æ ‡ä»£ç ã€‚

### 2.2 Tree-walk interpreters

æ ‘éå†è§£é‡Šå™¨

Some programming languages begin executing code right after parsing it to an AST (with maybe a bit of static analysis applied). To run the program, the interpreter traverses the syntax tree one branch and leaf at a time, evaluating each node as it goes.

This implementation style is common for student projects and little languages, but is not widely used for general-purpose languages since it tends to be slow. Some people use â€œinterpreterâ€ to mean only these kinds of implementations, but others define that word more generally, so Iâ€™ll use the inarguably explicit tree-walk interpreter to refer to these. Our first interpreter rolls this way.

è®¸å¤šçš„è¯­è¨€ï¼Œåœ¨è§£æé˜¶æ®µç”Ÿæˆäº†è¯­æ³•æ ‘åï¼Œå°±å¼€å§‹æ‰§è¡Œä»£ç ï¼ˆå¯èƒ½ä¼šä½¿ç”¨ä¸€äº›é™æ€åˆ†æï¼‰ã€‚ä¸ºäº†è¿è¡Œç¨‹åºï¼Œè§£é‡Šå™¨æ¯ä¸€æ¬¡éƒ½ä¼šéå†è¯­æ³•æ ‘çš„ä¸€ä¸ªåˆ†æ”¯å’Œå¶èŠ‚ç‚¹ï¼Œåœ¨æ¯ä¸€ä¸ªèŠ‚ç‚¹è¿è¡Œæ—¶å€™ï¼Œè¿›è¡Œè¯„ä¼°ã€‚

è¿™ç§å®ç°é£æ ¼ï¼Œåœ¨å­¦ç”Ÿä½œä¸šå’Œå°çš„è¯­è¨€ä¸­éå¸¸å¸¸è§ï¼Œä½†æ˜¯ï¼Œç”±äºè¿è¡Œé€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œæ²¡æœ‰å¹¿æ³›çš„åº”ç”¨äºé€šç”¨çš„è¯­è¨€ã€‚ä¸€äº›äººä½¿ç”¨è§£é‡Šå™¨ï¼Œè¡¨ç¤ºè¿™ç§ç±»å‹çš„å®ç°ï¼Œä½†æ˜¯ï¼Œå¦å¤–ä¸€äº›äººï¼Œä½¿ç”¨æ›´åŠ ä¸€èˆ¬çš„æœ¯è¯­æè¿°è¿™ç§å®ç°æ–¹å¼ï¼Œæœ¬ä¹¦ä¸­æˆ‘ä½¿ç”¨æ ‘éå†è§£é‡Šå™¨æ¥æè¿°è¿™ç§å®ç°ã€‚æˆ‘ä»¬ç¬¬ä¸€ä¸ªå®ç°Loxè¯­è¨€é‡‡ç”¨è¿™ç§å®ç°é£æ ¼ã€‚

> A notable exception is early versions of Ruby, which were tree walkers. At 1.9, the canonical implementation of Ruby switched from the original MRI (Matzâ€™s Ruby Interpreter) to Koichi Sasadaâ€™s YARV (Yet Another Ruby VM). YARV is a bytecode virtual machine.
>
> ä¸€ä¸ªè‘—åçš„ä¾‹å­æ˜¯ï¼ŒRubyè¯­è¨€çš„æ—©æœŸç‰ˆæœ¬ï¼Œä½¿ç”¨äº†æ ‘éå†é£æ ¼çš„å®ç°æ–¹å¼ã€‚åœ¨1.9ç‰ˆæœ¬ï¼ŒRubyçš„å®ç°ä»æ—©æœŸçš„MRIå˜æ›´ä¸ºYARVï¼ŒYARVæ˜¯ä¸€ä¸ªå­—èŠ‚ç è™šæ‹Ÿæœºã€‚

### 2.3 Transpilers

è½¬æ¢æœº

Writing a complete back end for a language can be a lot of work. If you have some existing generic IR to target, you could bolt your front end onto that. Otherwise, it seems like youâ€™re stuck. But what if you treated some other source language as if it were an intermediate representation?

Writing a complete back end for a language can be a lot of work. If you have some existing generic IR to target, you could bolt your front end onto that. Otherwise, it seems like youâ€™re stuck. But what if you treated some other source language as if it were an intermediate representation?

You write a front end for your language. Then, in the back end, instead of doing all the work to lower the semantics to some primitive target language, you produce a string of valid source code for some other language thatâ€™s about as high level as yours. Then, you use the existing compilation tools for that language as your escape route off the mountain and down to something you can execute.

They used to call this a source-to-source compiler or a transcompiler. After the rise of languages that compile to JavaScript in order to run in the browser, theyâ€™ve affected the hipster sobriquet transpiler.

ä¸ºä¸€ç§è¯­è¨€ç¼–å†™ä¸€ä¸ªå®Œæ•´çš„åç«¯ï¼Œéœ€è¦å¾ˆå¤šçš„å·¥ä½œã€‚å¦‚æœä½ æœ‰ä¸€äº›ç°æœ‰çš„é€šç”¨åç«¯ç›®æ ‡ï¼Œé‚£ä¹ˆä½ å¯ä»¥æŠŠå‰ç«¯è®¾è®¡ä¸ºé€šç”¨åç«¯çš„åŒ¹é…å‰ç«¯ã€‚å¦åˆ™çš„è¯ï¼Œä½ ä¼¼ä¹å¡åœ¨è¿™é‡Œäº†ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æŠŠå…¶ä»–çš„è¯­è¨€è§†ä¸ºä¸€ç§ä¸­é—´è¡¨ç¤ºï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥å¦‚ä½•åŒ¹é…ï¼Ÿ

ä½ ä¸ºè‡ªå·±çš„è¯­è¨€ç¼–å†™äº†å‰ç«¯ï¼Œç„¶åï¼Œåœ¨åç«¯ï¼Œä½ çš„æƒ³æ³•ä¸æ˜¯ï¼ŒæŠŠç”¨æˆ·ç¨‹åºçš„è¯­ä¹‰å˜æ›´ä¸ºåº•å±‚åŸå§‹çš„ç›®æ ‡è¯­è¨€ï¼Œè€Œæ˜¯æŠŠç”¨æˆ·è¯­ä¹‰è½¬æ¢ä¸ºä¸€ä¸ªæ›´é«˜çº§è¯­è¨€çš„æºä»£ç ã€‚ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ç§é«˜çº§è¯­è¨€çš„å·²ç»å­˜åœ¨çš„ç¼–è¯‘å™¨ï¼Œä½œä¸ºä¸‹å±±çš„å¤‡é€‰è·¯å¾„ã€‚

åœ¨ä»¥å‰ï¼Œè¿™ç§å®ç°æ–¹å¼è¢«ç§°ä¸ºæºä»£ç åˆ°æºä»£ç ç¼–è¯‘å™¨æˆ–è€… è½¬æ¢ç¼–è¯‘å™¨ï¼Œå½“å‡ºç°äº†ä¸€äº›è¯­è¨€ï¼Œä¸ºäº†èƒ½åœ¨æµè§ˆå™¨è¿è¡Œï¼Œï¼Œæœ€ç»ˆç¼–è¯‘ä¸ºJavaScriptåï¼Œå¤§å®¶æƒ³åˆ°äº†ä¸€ä¸ªæ–°çš„åç§° è½¬æ¢æœº æ¥æè¿°ã€‚

While the first transcompiler translated one assembly language to another, today, most transpilers work on higher-level languages. After the viral spread of UNIX to machines various and sundry, there began a long tradition of compilers that produced C as their output language. C compilers were available everywhere UNIX was and produced efficient code, so targeting C was a good way to get your language running on a lot of architectures.

Web browsers are the â€œmachinesâ€ of today, and their â€œmachine codeâ€ is JavaScript, so these days it seems almost every language out there has a compiler that targets JS since thatâ€™s the main way to get your code running in a browser.

The front endâ€”scanner and parserâ€”of a transpiler looks like other compilers. Then, if the source language is only a simple syntactic skin over the target language, it may skip analysis entirely and go straight to outputting the analogous syntax in the destination language.

If the two languages are more semantically different, youâ€™ll see more of the typical phases of a full compiler including analysis and possibly even optimization. Then, when it comes to code generation, instead of outputting some binary language like machine code, you produce a string of grammatically correct source (well, destination) code in the target language.

Either way, you then run that resulting code through the output languageâ€™s existing compilation pipeline, and youâ€™re good to go.

è™½ç„¶ç¬¬ä¸€ä¸ªè½¬æ¢æœºï¼ŒæŠŠä¸€ç§æ±‡ç¼–è¯­è¨€è½¬æ¢ä¸ºå¦å¤–ä¸€ç§æ±‡ç¼–è¯­è¨€ï¼Œä½†æ˜¯ç°åœ¨ï¼Œæˆ‘ä»¬å¸¸å¸¸æŠŠä¸€é—¨è¯­è¨€è½¬æ¢ä¸ºæ›´åŠ é«˜çº§çš„è¯­è¨€ã€‚å› ä¸ºUNIXçš„é£é¡ï¼Œç¼–è¯‘å™¨å¼€å§‹äº†ä¸€ä¸ªä¼ ç»Ÿï¼Œé‚£å°±æ˜¯æŠŠè½¬æ¢æœºè¾“å‡ºè¯­è¨€å˜ä¸º Cè¯­è¨€ã€‚Cè¯­è¨€ç¼–è¯‘å™¨ï¼Œå­˜åœ¨äºä»»æ„çš„UNIXç³»ç»Ÿä¸­ï¼Œæ”¯æŒæ›´å¤šçš„æ¶æ„ï¼Œå¹¶ä¸”Cç¼–è¯‘å™¨å¯ä»¥ç”Ÿæˆæ›´åŠ é«˜æ•ˆçš„ä»£ç ï¼Œå› æ­¤ï¼ŒæŠŠCè¯­è¨€ä½œä¸ºè¾“å‡ºç›®æ ‡ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ–¹å‘ã€‚

ç°åœ¨ï¼Œwebæµè§ˆå™¨æ˜¯ä¸€ç§æ–°å‹æœºå™¨ï¼Œè¿™ç§æ–°æœºå™¨çš„è¿è¡Œä»£ç æ˜¯JavaScriptï¼Œæ‰€ä»¥ç°åœ¨å¾ˆå¤šè¯­è¨€éƒ½æœ‰ä¸€ä¸ªé’ˆå¯¹JSçš„ç¼–è¯‘å™¨ï¼Œæœ€ç»ˆè¾“å‡ºJSä»£ç ï¼Œè¿™æ ·å¯ä»¥è®©æ–°çš„è¯­è¨€è¿è¡Œåœ¨æµè§ˆå™¨ä¸­ã€‚[è¯¦ç»†æ¸…å•](https://github.com/jashkenas/coffeescript/wiki/list-of-languages-that-compile-to-js)

è½¬æ¢æœºçš„å‰ç«¯éƒ¨åˆ†ï¼Œæ‰«æé˜¶æ®µã€è§£æé˜¶æ®µï¼Œå’Œå…¶ä»–çš„ç¼–è¯‘å™¨ç›¸ä¼¼ï¼Œç„¶è€Œï¼Œå¦‚æœæ–°è¯­è¨€åªæ˜¯ç›®æ ‡è¯­è¨€çš„ä¸€ä¸ªç®€å•çš®è‚¤ï¼Œé‚£ä¹ˆç¼–è¯‘å™¨å¯èƒ½ä¼šå®Œå…¨è·³è¿‡åˆ†æé˜¶æ®µï¼Œç›´æ¥è¾“å‡ºç›®æ ‡è¯­è¨€ã€‚

ä½†æ˜¯ï¼Œå¦‚æœè¿™ä¸¤ç§è¯­è¨€ï¼Œåœ¨è¯­ä¹‰ä¸Šéå¸¸ä¸ç›¸åŒï¼Œé‚£ä¹ˆï¼Œä½ å¯èƒ½ä¼šæ·»åŠ ä¸€äº›ä¼ ç»Ÿç¼–è¯‘å™¨çš„å…¶ä»–é˜¶æ®µï¼Œä¾‹å¦‚ï¼šåˆ†æé˜¶æ®µï¼Œä¼˜åŒ–é˜¶æ®µã€‚ä½†æ˜¯ï¼Œæœ€ç»ˆä»£ç ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬ä¸ä¼šç”Ÿæˆä¼ ç»Ÿçš„æœºå™¨ç æˆ–è€…å­—èŠ‚ç ï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªè¯­æ³•æ­£ç¡®çš„ç›®æ ‡è¯­è¨€å­—ç¬¦ä¸²ã€‚

æ— è®ºå¦‚ä½•ï¼Œä½ éƒ½å¯ä»¥åœ¨ç¼–è¯‘åï¼Œè·å–åˆ°ç›®æ ‡è¯­è¨€çš„æºä»£ç ï¼Œç„¶åé€šè¿‡ç›®æ ‡è¯­è¨€çš„ç¼–è¯‘å™¨ï¼Œå¼€å§‹è¿è¡Œæ–°è¯­è¨€ã€‚

> The first transcompiler, XLT86, translated 8080 assembly into 8086 assembly. That might seem straightforward, but keep in mind the 8080 was an 8-bit chip and the 8086 a 16-bit chip that could use each register as a pair of 8-bit ones. XLT86 did data flow analysis to track register usage in the source program and then efficiently map it to the register set of the 8086.
>
> It was written by Gary Kildall, a tragic hero of computer science if there ever was one. One of the first people to recognize the promise of microcomputers, he created PL/M and CP/M, the first high-level language and OS for them.
> 
> He was a sea captain, business owner, licensed pilot, and motorcyclist. A TV host with the Kris Kristofferson-esque look sported by dashing bearded dudes in the â€™80s. He took on Bill Gates and, like many, lost, before meeting his end in a biker bar under mysterious circumstances. He died too young, but sure as hell lived before he did.
>
>ç¬¬ä¸€ä¸ªè½¬æ¢æœºï¼ŒXLT86ï¼ŒæŠŠ8080 æ±‡ç¼–è½¬æ¢ä¸º8086æ±‡ç¼–ï¼Œè¿™çœ‹èµ·æ¥ä¼¼ä¹å¾ˆç®€å•ï¼Œä½†æ˜¯ï¼Œè¯·æ³¨æ„ï¼Œ8080æ˜¯ä¸€ä¸ª8ä½èŠ¯ç‰‡ï¼Œè€Œ8086æ˜¯ä¸€ä¸ª16ä½èŠ¯ç‰‡ï¼Œå¯ä»¥å°†8086çš„å¯„å­˜å™¨ï¼Œä½œä¸º8080çš„ä¸€å¯¹å¯„å­˜å™¨ä½¿ç”¨ï¼ŒXLT86è¿›è¡Œäº†æ•°æ®æµåˆ†æï¼Œç”¨äºè·Ÿè¸ªæºç¨‹åºä¸­çš„å¯„å­˜å™¨ä½¿ç”¨ï¼Œç„¶åï¼Œæœ‰æ•ˆçš„æŠŠè¿™äº›å¯„å­˜å™¨ï¼Œæ˜ å°„ä¸º8086çš„å¯„å­˜å™¨é›†ã€‚
>
> XLT86 æ˜¯ç”±Gary KIldallå®ç°çš„ï¼Œä»–æ˜¯è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ä¸€ä¸ªæ‚²å‰§äººç‰©ï¼Œä½œä¸ºæœ€æ—©è®¤è¯†åˆ°å¾®å‹è®¡ç®—æœºå‰æ™¯çš„äººä¹‹ä¸€ï¼Œä»–å¼€å‘äº†PL/M å’Œ CP/Mï¼Œè€ŒCP/Mæ˜¯ç¬¬ä¸€ç§å’Œæ“ä½œç³»ç»Ÿäº¤äº’çš„é«˜çº§è¯­è¨€ã€‚

>JS used to be the only way to execute code in a browser. Thanks to WebAssembly, compilers now have a second, lower-level language they can target that runs on the web.
>
>JSæ›¾ç»æ˜¯æµè§ˆå™¨ä¸­å”¯ä¸€çš„è¿è¡Œè¯­è¨€ï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬å¤šäº†ä¸€ç§é€‰æ‹© [WebAssembly](https://webassembly.org/), ç¼–è¯‘å™¨ç°åœ¨æ‹¥æœ‰äº†ç¬¬äºŒç§å¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨è¿è¡Œçš„è¯­è¨€ã€‚


### 2.4 Just-in-time compilation

å³æ—¶ç¼–è¯‘

This last one is less a shortcut and more a dangerous alpine scramble best reserved for experts. The fastest way to execute code is by compiling it to machine code, but you might not know what architecture your end userâ€™s machine supports. What to do?

You can do the same thing that the HotSpot Java Virtual Machine (JVM), Microsoftâ€™s Common Language Runtime (CLR), and most JavaScript interpreters do. On the end userâ€™s machine, when the program is loadedâ€”either from source in the case of JS, or platform-independent bytecode for the JVM and CLRâ€”you compile it to native code for the architecture their computer supports. Naturally enough, this is called just-in-time compilation. Most hackers just say â€œJITâ€, pronounced like it rhymes with â€œfitâ€.

The most sophisticated JITs insert profiling hooks into the generated code to see which regions are most performance critical and what kind of data is flowing through them. Then, over time, they will automatically recompile those hot spots with more advanced optimizations.

æœ€åä¸€ä¸ªä¸æ˜¯æ·å¾„ï¼Œè€Œæ˜¯å±é™©çš„é«˜å±±æ”€å²©ï¼Œæœ€å¥½ç•™ç»™ä¸“å®¶ã€‚æœ€å¿«çš„ä»£ç æ‰§è¡Œé€Ÿåº¦ï¼Œè‚¯å®šæ˜¯ç¿»è¯‘ä¸ºå…·ä½“çš„æœºå™¨ç ï¼Œä½†æ˜¯ï¼Œç¼–è¯‘é˜¶æ®µï¼Œä½ å¯èƒ½ä¸çŸ¥é“ç”¨æˆ·çš„æœºå™¨æ˜¯ä»€ä¹ˆæ¶æ„ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ

ä½ å¯ä»¥å€Ÿé‰´JVMï¼ˆJavaè¯­è¨€è™šæ‹Ÿæœºï¼‰ï¼ŒCLRï¼ˆå¾®è½¯çš„é€šç”¨è¯­è¨€è¿è¡Œåº“ï¼‰ï¼Œè¿˜æœ‰å¤§å¤šæ•°JSç¼–è¯‘å™¨åšçš„ï¼Œåœ¨ç”¨æˆ·çš„æœºå™¨ä¸Šï¼Œå½“ç¨‹åºåŠ è½½æ—¶å€™ï¼Œæ— è®ºæ˜¯ä»æºä»£ç ï¼Œè¿˜æ˜¯åœ¨JVM/CLRä¸­åŠ è½½å­—èŠ‚ç ï¼Œä½ å¯ä»¥å°†å…¶ç¼–è¯‘ä¸ºæœ¬æœºçš„æœºå™¨ç ã€‚å¾ˆè‡ªç„¶çš„ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºå³æ—¶ç¼–è¯‘ã€‚å¤§å¤šæ•°ç¼–ç¨‹é«˜æ‰‹ç§°è¿™ç§å®ç°ä¸ºJITï¼Œå‘éŸ³ç±»ä¼¼fitã€‚

æœ€å¤æ‚çš„JITï¼Œä¼šåœ¨ç”Ÿæˆä»£ç ä¸­æ’å…¥ä¸€äº›æ€§èƒ½åˆ†æä»£ç ï¼ŒæŸ¥çœ‹å“ªäº›ä»£ç å—ã€å“ªäº›æ•°æ®ç»“æ„å¯¹äºè¿è¡Œæ€§èƒ½å½±å“æœ€å¤§ã€‚ç„¶åï¼Œéšç€æ—¶é—´ç´¯ç§¯ï¼ŒJITå°†è‡ªåŠ¨ä½¿ç”¨æ›´é«˜çº§çš„ä¼˜åŒ–æ–¹å¼ï¼Œé‡æ–°ç¼–è¯‘çƒ­ç‚¹ä»£ç ã€‚

> This is, of course, exactly where the HotSpot JVM gets its name.
>
> å½“ç„¶ï¼Œè¿™ä¹Ÿæ˜¯JVMçš„ä¸€ç§å®ç°ï¼ŒHotSpot JVMçš„åç§°æ¥æºã€‚

## ä¸‰ã€Compilers and Interpreters

ç¼–è¯‘å™¨å’Œè§£é‡Šå™¨

Now that Iâ€™ve stuffed your head with a dictionaryâ€™s worth of programming language jargon, we can finally address a question thatâ€™s plagued coders since time immemorial: Whatâ€™s the difference between a compiler and an interpreter?

It turns out this is like asking the difference between a fruit and a vegetable. That seems like a binary either-or choice, but actually â€œfruitâ€ is a botanical term and â€œvegetableâ€ is culinary. One does not strictly imply the negation of the other. There are fruits that arenâ€™t vegetables (apples) and vegetables that arenâ€™t fruits (carrots), but also edible plants that are both fruits and vegetables, like tomatoes.

ç°åœ¨ï¼Œæˆ‘ä»¬å¤§è„‘ä¸­å·²ç»å¡æ»¡äº†å„ç§ç¼–è¯‘æœ¯è¯­ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è§£å†³ä¸€ä¸ªè‡ªå¤ä»¥æ¥å°±å›°æ‰°ç€ç¨‹åºå‘˜çš„é—®é¢˜ï¼Œç¼–è¯‘å™¨å’Œè§£é‡Šå™¨æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

å®é™…ä¸Šï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥ç±»æ¯”ä¸ºï¼Œæ°´æœå’Œè”¬èœæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿè¿™ä¸ªç­”æ¡ˆçœ‹èµ·æ¥æ˜¯ä¸€ä¸ªäºŒé€‰ä¸€é—®é¢˜ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œæ°´æœæ˜¯ä¸€ä¸ªæ¤ç‰©å­¦æœ¯è¯­ï¼Œè”¬èœåˆ™æ˜¯ä¸€ä¸ªçƒ¹é¥ªç”¨è¯­ï¼Œä¸€ä¸ªä¸œè¥¿æ˜¯æ°´æœå¹¶ä¸ä»£è¡¨å®ƒä¸å¯ä»¥æ˜¯è”¬èœï¼Œç°å®ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°æŸäº›æ°´æœï¼Œä¸æ˜¯è”¬èœï¼Œä¾‹å¦‚ï¼šè‹¹æœï¼›ä¹Ÿå¯ä»¥æ‰¾åˆ°æŸäº›è”¬èœï¼Œä¸å±äºæ°´æœï¼Œä¾‹å¦‚ï¼šèƒ¡èåœï¼›ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰¾åˆ°ï¼ŒæŸäº›ä¸œè¥¿ï¼Œå³æ˜¯æ°´æœï¼Œä¹Ÿæ˜¯è”¬èœï¼Œä¾‹å¦‚ï¼šè¥¿çº¢æŸ¿ã€‚

![plants](https://github.com/Kua-Fu/blog-book-images/blob/main/crafting-interpreters/plants.png?raw=true)

So, back to languages:

* Compiling is an implementation technique that involves translating a source language to some otherâ€”usually lower-levelâ€”form. When you generate bytecode or machine code, you are compiling. When you transpile to another high-level language, you are compiling too.

* When we say a language implementation â€œis a compilerâ€, we mean it translates source code to some other form but doesnâ€™t execute it. The user has to take the resulting output and run it themselves.

* Conversely, when we say an implementation â€œis an interpreterâ€, we mean it takes in source code and executes it immediately. It runs programs â€œfrom sourceâ€.

é‚£ä¹ˆï¼Œè¿”å›åˆ°è¯­è¨€éƒ¨åˆ†ï¼Œ

* ç¼–è¯‘æ˜¯ä¸€ç§å®ç°æŠ€æœ¯ï¼Œé€šå¸¸æ˜¯å°†æºè¯­è¨€è½¬æ¢ä¸ºæ›´åŠ ä½çº§åˆ«çš„å½¢å¼ï¼Œå½“ä½ æŠŠä¸€é—¨è¯­è¨€ç¼–è¯‘ä¸ºå­—èŠ‚ç æˆ–è€…æœºå™¨ç æ—¶å€™ï¼Œä½ ä½¿ç”¨äº†ç¼–è¯‘æŠ€æœ¯ï¼Œå½“ä½ è½¬æ¢ä¸ºå¦å¤–ä¸€ç§é«˜çº§è¯­è¨€æ—¶å€™ï¼Œä½ ä¹Ÿåœ¨ä½¿ç”¨ç¼–è¯‘ã€‚

* å½“æˆ‘ä»¬è¯´å®ç°äº†ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œæˆ‘ä»¬çš„æ„æ€æ˜¯ï¼Œå°†æºè¯­è¨€è½¬æ¢ä¸ºå…¶ä»–å½¢å¼ï¼Œä½†æ˜¯å¹¶ä¸æ‰§è¡Œï¼Œç”¨æˆ·éœ€è¦è·å–åˆ°ç¼–è¯‘ç»“æœï¼Œç„¶åå†è¿è¡Œã€‚

* å¯¹åº”çš„ï¼Œå½“æˆ‘ä»¬è¯´å®ç°äº†ä¸€ä¸ªè§£é‡Šå™¨ï¼Œæˆ‘ä»¬çš„æ„æ€æ˜¯ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œæºä»£ç ï¼Œçœ‹èµ·æ¥ï¼Œæˆ‘ä»¬å¥½åƒæ˜¯ç›´æ¥ä»æºè¯­è¨€è¿è¡Œã€‚

Like apples and oranges, some implementations are clearly compilers and not interpreters. GCC and Clang take your C code and compile it to machine code. An end user runs that executable directly and may never even know which tool was used to compile it. So those are compilers for C.

In older versions of Matzâ€™s canonical implementation of Ruby, the user ran Ruby from source. The implementation parsed it and executed it directly by traversing the syntax tree. No other translation occurred, either internally or in any user-visible form. So this was definitely an interpreter for Ruby.

>Peanuts (which are not even nuts) and cereals like wheat are actually fruit, but I got this drawing wrong. What can I say, Iâ€™m a software engineer, not a botanist. I should probably erase the little peanut guy, but heâ€™s so cute that I canâ€™t bear to.
> 
> Now pine nuts, on the other hand, are plant-based foods that are neither fruits nor vegetables. At least as far as I can tell.


åƒè‹¹æœå’Œæ©˜å­ä¸€æ ·ï¼Œå®ƒä»¬æ˜¯æ°´æœä½†ä¸æ˜¯è”¬èœï¼Œæœ‰ä¸€äº›å®ç°ï¼Œæ˜¯ç¼–è¯‘å™¨ï¼Œè€Œä¸æ˜¯è§£é‡Šå™¨ã€‚GCCå’ŒClang æ¥æ”¶åŸå§‹çš„Cè¯­è¨€ç¨‹åºï¼Œæœ€ç»ˆç¼–è¯‘ä¸ºæœºå™¨ç ã€‚ç”¨æˆ·æœ€ç»ˆè¿è¡Œå¯æ‰§è¡Œæ–‡ä»¶ï¼Œè€Œä¸éœ€è¦çŸ¥é“å…·ä½“ä½¿ç”¨äº†å“ªä¸ªç¼–è¯‘å™¨ï¼Œå®ƒä»¬éƒ½æ˜¯Cè¯­è¨€ç¼–è¯‘å™¨ã€‚

Rubyçš„è€ç‰ˆæœ¬ä¸­ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä»Rubyæºç è¿è¡Œã€‚Rubyè§£é‡Šå™¨ç›´æ¥è§£ææºç¨‹åºï¼Œç”Ÿæˆè¯­æ³•æ ‘ï¼Œç„¶åï¼Œéå†è¯­æ³•æ ‘ï¼Œç›´æ¥æ‰§è¡Œï¼Œæ— è®ºå¤„äºç”¨æˆ·è§’åº¦ï¼Œè¿˜æ˜¯å®é™…å†…éƒ¨æœºåˆ¶ï¼Œéƒ½æ²¡æœ‰å…¶ä»–çš„è½¬æ¢è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šè¿™ç§å®ç°ä¸ºè§£é‡Šå™¨ã€‚


But what of CPython? When you run your Python program using it, the code is parsed and converted to an internal bytecode format, which is then executed inside the VM. From the userâ€™s perspective, this is clearly an interpreterâ€”they run their program from source. But if you look under CPythonâ€™s scaly skin, youâ€™ll see that there is definitely some compiling going on.

The answer is that it is both. CPython is an interpreter, and it has a compiler. In practice, most scripting languages work this way, as you can see:

![venn](https://github.com/Kua-Fu/blog-book-images/blob/main/crafting-interpreters/venn.png?raw=true)

ä½†æ˜¯ï¼ŒCPythonæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå½“æˆ‘ä»¬è¿è¡ŒPythonç¨‹åºæ—¶å€™ï¼ŒCPythonç¼–è¯‘å™¨å°†Pythonä»£ç è½¬æ¢ä¸ºå†…éƒ¨çš„å­—èŠ‚ç ï¼Œåœ¨Pythonè™šæ‹Ÿæœºä¸­è¿è¡Œã€‚ä»ç”¨æˆ·è§’åº¦çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªè§£é‡Šå™¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´æ¥ä»åŸå§‹ä»£ç è¿è¡Œï¼Œä½†æ˜¯å¦‚æœä»å†…éƒ¨å®ç°è§’åº¦ï¼Œè¿˜å­˜åœ¨ç€ä¸€äº›ç¼–è¯‘å™¨ã€‚

å‡†ç¡®ç­”æ¡ˆæ˜¯ï¼ŒCPythonæ—¢æ˜¯ä¸€ä¸ªè§£é‡Šå™¨ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œå®é™…ä¸Šï¼Œå¤§éƒ¨åˆ†çš„è„šæœ¬è¯­è¨€éƒ½æ˜¯è¿™æ ·çš„

That overlapping region in the center is where our second interpreter lives too, since it internally compiles to bytecode. So while this book is nominally about interpreters, weâ€™ll cover some compilation too.

å¦‚ä¸Šå›¾ï¼Œæˆ‘ä»¬ç¬¬äºŒéƒ¨åˆ†å®ç°çš„Loxè§£é‡Šå™¨ï¼Œå¤„äºä¸­é—´çš„é‡å éƒ¨åˆ†ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šç”Ÿæˆå­—èŠ‚ç ã€‚å› æ­¤ï¼Œè™½ç„¶æœ¬ä¹¦æ˜¯å…³äºè§£é‡Šå™¨ä»‹ç»çš„ï¼Œä½†æ˜¯ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¼šæ¶‰åŠåˆ°ç¼–è¯‘å™¨çš„å†…å®¹ã€‚

>The Go tool is even more of a horticultural curiosity. If you run go build, it compiles your Go source code to machine code and stops. If you type go run, it does that, then immediately executes the generated executable.
>
> So go is a compiler (you can use it as a tool to compile code without running it), is an interpreter (you can invoke it to immediately run a program from source), and also has a compiler (when you use it as an interpreter, it is still compiling internally).
>
> goè¯­è¨€çš„å‘½ä»¤æ›´åŠ èƒ½è¯´æ˜é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæ‰§è¡Œå‘½ä»¤ go buildï¼Œæˆ‘ä»¬èƒ½å¾—åˆ°ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ï¼Œå¦‚æœæ‰§è¡Œå‘½ä»¤ï¼Œgo runï¼Œç¨‹åºä¼šé©¬ä¸Šè¿è¡Œã€‚
>
> æ‰€ä»¥ï¼Œgoæ˜¯ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œå¯ä»¥å°†goç¨‹åºç¼–è¯‘ä¸ºå¯æ‰§è¡Œæ–‡ä»¶ï¼Œgoä¹Ÿæ˜¯ä¸€ä¸ªè§£é‡Šå™¨ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œgoç¨‹åºï¼Œä½†æ˜¯è§£é‡Šå™¨ä¸­è¿˜åŒ…å«ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œå½“ç›´æ¥è¿è¡Œgoç¨‹åºæ—¶å€™ï¼Œå†…éƒ¨ä»æœ‰ç¼–è¯‘æ­¥éª¤ã€‚

## å››ã€Our Journey

æˆ‘ä»¬çš„æ—…é€”

Thatâ€™s a lot to take in all at once. Donâ€™t worry. This isnâ€™t the chapter where youâ€™re expected to understand all of these pieces and parts. I just want you to know that they are out there and roughly how they fit together.

This map should serve you well as you explore the territory beyond the guided path we take in this book. I want to leave you yearning to strike out on your own and wander all over that mountain.

But, for now, itâ€™s time for our own journey to begin. Tighten your bootlaces, cinch up your pack, and come along. From here on out, all you need to focus on is the path in front of you.

æœ¬ç« ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¾ˆå¤šå†…å®¹ï¼Œåˆ«æ‹…å¿ƒï¼Œä½ ä¸éœ€è¦ç°åœ¨å°±ç†è§£æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬åªæ˜¯ï¼Œå…ˆä»‹ç»å®ƒä»¬ï¼Œä½ éœ€è¦çŸ¥é“å®ƒä»¬æ˜¯å­˜åœ¨çš„ï¼Œå¹¶ä¸”éœ€è¦çŸ¥é“å®ƒä»¬æ˜¯å¦‚ä½•ç»“åˆåœ¨ä¸€èµ·çš„ã€‚

æœ¬ç« æ¶‰åŠçš„åœ°å›¾å°†å¾ˆå¥½çš„é™ªä¼´ç€ä½ ï¼Œå› ä¸ºå®ƒåŒ…å«æœ‰ä¸€äº›å†…å®¹ï¼Œæœ¬ä¹¦ä¸­ä¸ä¼šæ¶‰åŠåˆ°ã€‚æˆ‘æƒ³è¦ç¦»å¼€ä½ ï¼Œè®©ä½ ç‹¬è‡ªå»æ¢ç´¢ã€äº«å—çˆ¬å±±çš„è¿‡ç¨‹ã€‚

ä½†æ˜¯ï¼Œç°åœ¨è¿˜ä¸æ˜¯ç‹¬è‡ªæ¢ç´¢çš„æ—¶å€™ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å¼€å¯æ—…ç¨‹ã€‚ç³»ç´§é‹å¸¦ï¼Œæ”¶å¥½èƒŒåŒ…ï¼Œè·Ÿä¸Šæ¥ï¼Œä»ç°åœ¨å¼€å§‹ï¼Œä½ éœ€è¦å…³æ³¨çœ¼å‰çš„é“è·¯ã€‚

> Henceforth, I promise to tone down the whole mountain metaphor thing.
> 
> ä»ä»Šå¾€åï¼Œæˆ‘ä¼šæ·¡åŒ–çˆ¬å±±è¿™ä»¶äº‹æƒ…ã€‚

## äº”ã€CHALLENGES

ä¹ é¢˜é›†

1. Pick an open source implementation of a language you like. Download the source code and poke around in it. Try to find the code that implements the scanner and parser. Are they handwritten, or generated using tools like Lex and Yacc? (.l or .y files usually imply the latter.)

   é€‰æ‹©ä¸€ç§ä½ ç†Ÿæ‚‰ã€å–œæ¬¢çš„å¼€æºè¯­è¨€ï¼Œä¸‹è½½æºç ç„¶åæµè§ˆä¸€ä¸‹ï¼Œå°è¯•æ‰¾å‡ºå…¶ä¸­çš„æ‰«æå™¨ã€è§£æå™¨éƒ¨åˆ†ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯è‡ªå·±å®ç°çš„ï¼Œè¿˜æ˜¯ä½¿ç”¨Lex/Yaccç­‰ç¼–è¯‘å™¨å·¥å…·å®ç°çš„ï¼Œå¯ä»¥æŸ¥çœ‹æ˜¯å¦å­˜åœ¨ .l, .y åç¼€çš„æ–‡ä»¶ï¼Œè¿™é€šå¸¸æ˜¯å·¥å…·ç”Ÿæˆæ–‡ä»¶ã€‚
   
   
1. Just-in-time compilation tends to be the fastest way to implement dynamically typed languages, but not all of them use it. What reasons are there to not JIT?

	å³æ—¶ç¼–è¯‘é€šå¸¸æ˜¯åŠ¨æ€è¯­è¨€æœ€å¿«çš„å®ç°æ–¹å¼ï¼Œä½†æ˜¯å¹¶éæ‰€æœ‰çš„è¯­è¨€éƒ½åˆ©ç”¨è¿™ç§ç‰¹æ€§ï¼Œä¸ºä»€ä¹ˆå®ƒä»¬ä¸æä¾›å³æ—¶ç¼–è¯‘ï¼Ÿ
	
1. Most Lisp implementations that compile to C also contain an interpreter that lets them execute Lisp code on the fly as well. Why?

	å¤§å¤šæ•°çš„Lispå®ç°ï¼Œåœ¨å®ç°ä¸€ä¸ªç¼–è¯‘å™¨ï¼Œç¼–è¯‘ä¸ºCè¯­è¨€çš„åŒæ—¶ï¼Œè¿˜æä¾›ä¸€ä¸ªè§£é‡Šå™¨ï¼Œä¿è¯å¯ä»¥åŠ¨æ€æ‰§è¡ŒLispä»£ç ï¼Œä¸ºä»€ä¹ˆï¼Ÿ



























