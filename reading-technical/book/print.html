<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>reading-technical</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4717236929129160" crossorigin="anonymous"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MMN1K84KRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-MMN1K84KRS');
</script>

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Alogrithm</li><li class="chapter-item expanded "><a href="hyperloglog.html"><strong aria-hidden="true">2.</strong> hyperloglog</a></li><li class="chapter-item expanded "><a href="logmine.html"><strong aria-hidden="true">3.</strong> LogMine: Fast Pattern Recognition for Log Analytics</a></li><li class="chapter-item expanded "><a href="percentile.html"><strong aria-hidden="true">4.</strong> percentile pre-calculation</a></li><li class="chapter-item expanded affix "><li class="part-title">Elasticsearch</li><li class="chapter-item expanded "><a href="elasticsearch.html"><strong aria-hidden="true">5.</strong> Elasticsearch 101</a></li><li class="chapter-item expanded affix "><li class="part-title">Task Framework</li><li class="chapter-item expanded "><a href="asynq.html"><strong aria-hidden="true">6.</strong> asynq</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="asynq/dropbox_atf.html"><strong aria-hidden="true">6.1.</strong> Dropbox ATF</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">reading-technical</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hyperloglog"><a class="header" href="#hyperloglog">hyperloglog</a></h1>
<h2 id="一参考"><a class="header" href="#一参考">一、参考</a></h2>
<blockquote>
<p><a href="https://towardsdatascience.com/hyperloglog-a-simple-but-powerful-algorithm-for-data-scientists-aed50fe47869">HyperLogLog: A Simple but Powerful Algorithm for Data Scientists</a></p>
</blockquote>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Philippe_Flajolet">Philippe Flajolet</a></p>
</blockquote>
<blockquote>
<p><a href="https://github.com/Kua-Fu/blog-book-images/blob/main/paper/FlMa85.pdf">Pro ba bit istic Cou nting Algorithms for Data Base Applications </a></p>
</blockquote>
<blockquote>
<p><a href="https://github.com/Kua-Fu/blog-book-images/blob/main/paper/DuFl03-LNCS.pdf">Loglog Counting of Large Cardinalities (Extended Abstract)</a></p>
</blockquote>
<blockquote>
<p><a href="https://github.com/Kua-Fu/blog-book-images/blob/main/paper/FlFuGaMe07.pdf">HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm</a></p>
</blockquote>
<h2 id="二论文"><a class="header" href="#二论文">二、论文</a></h2>
<p>Probabitistic Counting Algorithms for Data Base Applications</p>
<p>数据库应用的概率计数算法</p>
<p>This paper introduces a class of probabilistic counting algorithms with which one can estimate the number of distinct elements in a large collection of data (typically a large file stored on disk) in a single pass using only a small additional storage (typically less than a hundred binary words) and only a few operations per element scanned. </p>
<p>The algorithms are based on statistical observations made on bits of hashed values of records. They are by construction totally insensitive to the replicative structure of elements in the file; they can be used in the context of distributed systems without any degradation of performances and prove especially useful in the context of data bases query optirnisation.</p>
<p>本文介绍了一类概率计数算法，通过该算法，可以在一次扫描中，仅仅使用少量的额外存储（通常少于100个二进制字），通过对每一个扫描元素进行少量的操作，就可以估算大量数据中不同元素的数量（数据通常是保存在磁盘上的大文件）</p>
<p>该算法，基于对记录散列值的比特进行的统计观察。通过构造，它们对文件中元素的复制结构完全不敏感，它们可以在分布式系统的环境中使用，而不会降低性能，并且在数据库查询优化的环境中特别有效。</p>
<h3 id="21-introduction"><a class="header" href="#21-introduction">2.1 INTRODUCTION</a></h3>
<p>介绍</p>
<p>As data base systems allow the user to specify more and more complex queries, the need arises for efficient processing methods. A complex query can however generally be evaluated in a number of different manners, and the overall performance of a data base system depends rather crucially on the selection of appropriate decomposition strategies in each particular case.</p>
<p>因为数据库系统允许用户指定越来越复杂的查询，因此，需要更加高效的处理方法。然而，复杂的查询通常可以以多种不同的方式评估，数据库系统的总体性能，关键取决于在每个特定情况下，选择适当的分散策略。</p>
<p>Even a problem as trivial as computing the intersection of two collections of data A and B lends itself to a number of different treatments: </p>
<ol>
<li>
<p>Sort A, search each element of B in A and retain it if it appears in A;</p>
</li>
<li>
<p>sort A, sort B, then perform a merge-like operation to determine the intersection;</p>
</li>
<li>
<p>eliminate duplicates in A and/or B using hashing or hash filters, then perform Algorithm 1 or 2</p>
</li>
</ol>
<p>即使像是计算两个数据集合 A、B的交集这样一个简单的问题，也有许多不同的方法</p>
<ol>
<li>
<p>先把 A 排序，遍历 B 中的每一个元素，判断是否存在于A中，如果存在，则保留该元素;</p>
</li>
<li>
<p>排序A，排序B，然后执行一个类似于 merge 的操作，计算出交集;</p>
</li>
<li>
<p>先利用hash算法，将A和B集合去重，然后，在使用1或者2中的方法计算交集</p>
</li>
</ol>
<p>Each of these evaluation strategy will have a cost essentially determined by the number of records a, b in A and B, and the number of distinct elements α, β in A and B, and for typical sorting methods, the costs are:</p>
<p>for strategy 1: \( O(a \cdot log\alpha + b \cdot log\beta) \)</p>
<p>for strategy 2: \( O(a \cdot log a + b \cdot log b + a + b) \) </p>
<p>In a number of similar situations, it appears thus that, apart from the sizes of the files on which one operates (i.e., the number of records), a major determinant of efficiency is the cardinalities of the underlying sets, i.e., the number of distinct elements they comprise.</p>
<p>The situation gets much more complex when operations like projections, selections, multiple joins in combination with various boolean operations appear in queries. As an example, the relational system system R has a sophisticated query optimiser. In order to perform its task, that programme keeps several statistics on relations of the data base.The most important ones are the sizesof relations as well as the number of different elements of some key fields. The choices are made in order to minimise a cer- tain cost function that depends on specific CPU and disk access costs as well as sizes and cardinalities of relations or fields. In system R, this information is periodically recomputed and kept in catalogues that are companions to the data base records and indexes.</p>
<p>上面每一种计算方法的性能，基本上取决于集合A、B中的元素a, b 的数量，和集合A、B中的不相同元素α, β的数量，对于典型的排序方法，复杂度如下</p>
<p>因此，在很多类似场景下，似乎除了操作文件的大小（即记录数量）之外，效率的主要决定因素是——基础集合的基数，即集合中包含的不同元素的个数。</p>
<p>当查询中出现了取子集，取具体列，或者多重join操作，以及布尔运算，查询情况将变得更加复杂。例如，关系型系统 R 有一个复杂的查询优化器。为了执行该任务，该方案保留了关于数据库关系的若干统计数据。最重要的是，关系的大小和一些关键字段的不同元素的数量。这些信息，用于确定在任何给定时间属性的选择，以便决定键的选择，以及计算关系运算符时候，选择适当的算法。这些选择是为了最小化特定的成本函数，该函数取决于特定的CPU和磁盘访问成本，已经关系或字段的大小和基数。在系统R中，该信息会定期更新，并且保存在数据库记录和索引的目录中。</p>
<p>In this paper, we propose efficient algorithms to estimate the cardinalities of multisets of data as are commonly encountered in data base practice. A trivial method consists in determining card(M) by building a list of all elements of M without replication; this method has the advantage of being exact but it has a cost in number of disk accessesand auxiliary storage (at least \( O(a) \) or \( O(a \cdot log a \) ) if sorting is used) that might be higher than the possible gains which one can obtain using that information.</p>
<p>在本文中，我们提出了一个有效的算法来估计数据库实践中常见的多个数据集的基数。一个简单的方法是，通过创建一个没有重复值的集合M 来计算 \(card(M) \) ; 该方法的优点是计算值准确，但其磁盘访问和额外存储（如果使用排序，则至少是 \( O(a) \) 或者 \( a \cdot log(a) \)) 的成本可能高于使用该去重值 可以获得的收益。</p>
<p>The method we propose here is probabilistic in nature since its result depends on the particular hashing function used and on the particular data on which it operates. It uses minimal extra storage in core and provides practically useful estimates on cardinalities of large collections of data. The accuracy is inversely related to the storage: using 64 binary words of typically 32 bits, we attain a typical accuracy of 10%; using 256 words, the accuracy improves to about 5%. The performances do not degrade as files get large: with 32 bit words, one can safely count cardinalities well over 100 million. Furthermore, by design, our algorithms are totally insensitive to the replication structures of files: as opposed to sampling techniques,  the result will be the same whether elements appear a million times or just a few times.</p>
<p>我们在这里提出的方法，本质上是概率性的，因为它的结果取决于所使用的特定哈希函数和它所操作的特定数据。它的设计核心是使用最少的额外存储，并对大量数据集合的基数提供了实际有用的估计。准确度和额外存储用量成反比，如果使用64个二进制字（每一个都占用32位bit），我们可以获得10%的准确性。如果提高到256个二进制字，准确度相应提高位 5%，当文件变大时候，性能不会降低。使用32个二进制字，用户可以安全的计算超过1亿基数的集合。这个方法的唯一假设是，我们可以用适当方法对记录进行散列，实现伪统一性。然而，这不是一个严重的限制，因为对大型工业文件的实证研究表明，标准哈希技术的谨慎实施确实实现了哈希值的实际一致性。此外，根据设计，我们的算法对于重复的文件结构，不敏感，与采样技术相反：无论元素出现了一百万次，还是出现了几次，基数计算结果都是一样的。</p>
<p>From a more theoretical standpoint, these techniques constitute yet another illustration of the gains that may be achieved in many situations through the use of probabilistic methods. We mention here Morris’ approximate counting algorithm  which maintains approximate counters with an expected constant relative accuracy using only</p>
<p>\( log2 \cdot log_2^n + O(1) \)</p>
<p>bits in order to count up to n. Morris’ algorithm may be used to reduce by a factor of 2 the memory size necessary to store large statistics on a large number of events in computer systems.</p>
<p>从更加理论的角度看，这些技术构成了通过使用概率方法在许多情况下可能实现收益的另一个例证。这里我们提到了莫里斯的近似计数算法，该算法保持近似计数通过一个可以预测的相对精度. 通过  \( log2 \cdot log_2^n + O(1) \) 个bit，可以估算基数为 n. 莫里斯算法，可以将存储在计算机中的大量事件的大量统计数据，所需要的存储器大小减少2倍。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logmine-fast-pattern-recognition-for-log-analytics"><a class="header" href="#logmine-fast-pattern-recognition-for-log-analytics">LogMine: Fast Pattern Recognition for Log Analytics</a></h1>
<p>logMine: 日志分析的快速识别模式</p>
<h2 id="参考"><a class="header" href="#参考">参考</a></h2>
<blockquote>
<p><a href="https://github.com/trungdq88/logmine">python logmine</a></p>
</blockquote>
<blockquote>
<p><a href="https://www.cs.unm.edu/%7Emueen/Papers/LogMine.pdf">logmine 论文</a></p>
</blockquote>
<blockquote>
<p><a href="https://github.com/Kua-Fu/blog-book-images/blob/main/paper/LogMine.pdf">logmine 论文下载</a></p>
</blockquote>
<h2 id="abstract"><a class="header" href="#abstract">ABSTRACT</a></h2>
<p>概要</p>
<p>Modern engineering incorporates smart technologies in all aspects of our lives. Smart technologies are generating terabytes of log messages every day to report their status. </p>
<p>It is crucial to analyze these log messages and present usable information (e.g. patterns) to administrators, so that they can manage and monitor these technologies. Patterns minimally represent large groups of log messages and enable the administrators to do further analysis, such as anomaly detection and event prediction. </p>
<p>Although patterns exist commonly in automated log messages, recognizing them in massive set of log messages from heterogeneous sources without any prior information is a significant undertaking. We propose a method, named LogMine, that extracts high quality patterns for a given set of log messages. Our method is fast, memory efficient, accurate, and scalable. </p>
<p>LogMine is implemented in map-reduce framework for distributed platforms to process millions of log messages in seconds. LogMine is a robust method that works for heterogeneous log messages generated in a wide variety of systems. Our method exploits algorithmic techniques to minimize the computational overhead based on the fact that log messages are always automatically generated. We evaluate the performance of LogMine on massive sets of log messages generated in industrial applications. LogMine has successfully generated patterns which are as good as the patterns generated by exact and unscalable method, while achieving a 500× speedup. </p>
<p>Finally, we describe three applications of the patterns generated by LogMine in monitoring large scale industrial systems.</p>
<p>在我们生活的方方面面，现代工程都融入大量的智能应用。智能应用每天都会生成TB级别的日志，反应运行状态。</p>
<p>快速分析这些应用日志，并且向管理员提供可用信息非常重要，管理员可以管理和监控这些智能应用。模式最低限度的描述了大量的日志信息，使得管理员可以进一步分析日志，例如：异常检测和事件预测。</p>
<p>虽然，模式存在于自动采集的日志中，但是在没有任何先验信息的情况下，对来自很多不同应用的大量日志，识别分类非常有挑战性。我们提出一种称为 logmine的方法，该方法能提取给定日志的高质量模式，我们的logmine方法，是快速、内存高效、准确和可扩展的。</p>
<p>logmine是基于map-reduce 框架实现的，在分布式平台中，可以在数秒时间处理数百万条日志。logmine是一种健壮方法，适用于多个不同应用的日志处理。基于日志总是自动生成、采集的前提，我们将不断优化算法，最小化占用系统资源。 我们评估了 logmine 对工业应用中的大量日志处理的性能。 一方面，logmine可以生成精确不可缩放方法中一样多的模型，另一方面，logmine的生成速度是普通方法的500多倍。</p>
<p>最后，我们将描述基于生成模式，在监控大规模工业系统中的三个应用。</p>
<h2 id="1-introduction"><a class="header" href="#1-introduction">1. INTRODUCTION</a></h2>
<p>介绍</p>
<p>The Internet of Things (IoT) enables advanced connectivity of computing and embedded devices through internet infrastructure. Although computers and smartphones are the most common devices in IoT, the number of “things” is expected to grow to 50 billion by 2020 </p>
<p>IoT involves machine-to-machine communications (M2M), where it is important to continuously monitor connected machines to detect any anomaly or bug, and resolve them quickly to minimize the downtime. Logging is a commonly used mechanism to record machines’ behaviors and various states for maintenance and troubleshooting. An acceptable logging standard is yet to be developed for IoT, most commonly due to the enormous varieties of “things” and their fast evolution over time. Thus, it is extremely challenging to parse and analyze log messages from systems like IoT.</p>
<p>基于互联网基础设施，物联网实现了计算和嵌入式设备之间的关联。虽然计算机和手机是最常见的设备，但是物联网中不仅仅只有它们，预计2020年，将会有500亿个智能设备。</p>
<p>物联网涉及机器间的通信，在通信过程中，持续监控连接的机器，监测它们可能发生的任何异常和报错，并且最快时间的解决异常问题，减少设备停机时间，非常重要。日志记录是一种常用机制，日志记录机器的行为和各种状态，便于后期维护和故障排除。物联网至今还没有制定一个可以接受的日志标准，因为智能设备种类非常多、随时间更新频繁。因此，解析和分析来自物联网系统的大量日志，非常有挑战性。</p>
<p>An automated log analyzer must have one component to recognize patterns from log messages, and another component to match these patterns with the inflow of log messages to identify events and anomalies. Such a log message analyzer must have the following desirable properties:</p>
<p>自动日志分析器必须有一个组件来识别日志消息中的模式，另一个组件将这些组件和日志流进行匹配，用于识别事件和异常。这一类日志识别分析器，需要具备下面的特性：</p>
<ol>
<li>
<p>No-supervision: </p>
<p>The pattern recognizer needs to be working from the scratch without any prior knowledge or human supervision. For a new log message format, the pattern recognizer should not require an input from the administrator.</p>
<p>无监督，模式识别器需要在没有任何先验知识或者人工监督的情况下从头开始，对于新的日志格式，模式识别器不需要管理员的输入。</p>
</li>
<li>
<p>Heterogeneity: </p>
<p>There can be log messages generated from different applications and systems. Each system may generate log messages in multiple formats. An automated recognizer must find all formats of the log messages irrespective of their origins.</p>
<p>异构性，可以从不同的应用程序和系统生成日志，每个系统都可以生成多种格式的日志，自动识别器需要识别日志的所有格式，而不考虑日志的来源</p>
</li>
<li>
<p>Efficiency: </p>
<p>IoT-like systems generate millions of log messages every day. The log processing should be done so efficiently that the processing rate is always faster than the log generation rate.</p>
<p>效率，类似物联网的系统，每天生成数百万条日志，日志处理需要高效进行，处理的效率要大于日志产生的速度。</p>
</li>
<li>
<p>Scalability: </p>
<p>Pattern recognizer must be able to pro- cess massive batches of log messages to maintain a cur- rent set of patterns without incurring CPU and mem- ory bottlenecks.</p>
<p>可扩展性，模式识别器必须能够处理大量的日志，维护当前的模式集，不会产生CPU和内存瓶颈。</p>
</li>
</ol>
<p>Many companies such as Splunk, Sumo Logic, Loggly, LogEntries, etc. offer log analysis tools. Open source packages such as ElasticSearch, Graylog and OSSIM have also been developed to analyze logs. Most of these tools and packages use regular expressions (regex) to match with log messages. These tools assume that the ad- ministrators know how to work with regex, and there are plenty of tools and libraries that support regex. However, these tools do not have the desirable properties mentioned earlier. </p>
<p>By definition, these tools support only supervised matching. Human involvement is clearly non-scalable for heterogeneous and continuously evolving log message formats in systems such as IoT, and it is humanly impossible to parse the sheer number of log entries generated in an hour, let alone days and weeks. </p>
<p>On top of that, writing regex rules is long, frustrating, error-prone, and regex rules may conflict with each other especially for IoT-like systems. Even if a set of regex rules is written, the rate of processing log messages can be slow due to overgeneralized regexes.</p>
<p>许多公司都提供日志分析工具，例如：Splunk, sumo logic, loggly, logEntries 等等。还有很多开源软件，也可以分析日志，例如：ElasticSearch、Graylog 和 OSIM 等。大多数工具和包使用正则表达式 与日志消息匹配。这些工具假设管理员知道如何写正则表达式，并且有很多工具和库支持正则表达式。然而，这些工具都不具备前文的特性。</p>
<p>根据定义，这些工具是监督匹配。对于物联网系统中各种不同格式、不断更新的日志，人类参与分类显然是不可行的，从人类角度，不可能把一个小时的日志分类，更不用说一天，几天，几周。</p>
<p>除此之外，编写正则规则，容易导致规则非常长，让人困惑，编写出错等问题。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="percentile-pre-calculation"><a class="header" href="#percentile-pre-calculation">percentile pre-calculation</a></h1>
<h2 id="一参考-1"><a class="header" href="#一参考-1">一、参考</a></h2>
<blockquote>
<p><a href="https://github.com/Kua-Fu/blog-book-images/blob/main/docs/ddSketch.pdf">DDSketch: A Fast and Fully-Mergeable Quantile Sketch with Relative-Error Guarantees</a></p>
</blockquote>
<blockquote>
<p><a href="https://www.datadoghq.com/blog/engineering/computing-accurate-percentiles-with-ddsketch/?_gl=1*tc89tr*_ga*NzA2MjcyNzYyLjE2MjgwNjc5NDc.*_ga_KN80RDFSQK*MTY1MTEzMjMxOS4yNTYuMS4xNjUxMTM2NTgxLjA.#how-accurate-are-those-percentiles">Computing Accurate Percentiles with DDSketch</a></p>
</blockquote>
<blockquote>
<p><a href="https://github.com/Kua-Fu/blog-book-images/blob/main/docs/quantiles.pdf">Space-Efficient Online Computation of Quantile Summaries</a></p>
</blockquote>
<h2 id="二设计背景"><a class="header" href="#二设计背景">二、设计背景</a></h2>
<p>在 <code>DataFlux</code>，每天都需要处理大量的分布式数据，如果直接在请求时，实时计算百分位值，会非常消耗资源，甚至请求超时。 需要找到一个好的算法，可以满足下面的几个要求，高效的计算百分位值。</p>
<ol>
<li>
<p>保证计算结果，在一定范围内可靠，即存在相对误差；</p>
</li>
<li>
<p>计算消耗资源少，即 <code>cpu/memory</code> 的消耗在一定可控范围；</p>
</li>
<li>
<p>可以分布式计算，充分利用集群分布式性能</p>
</li>
</ol>
<h2 id="三为什么百分位计算特殊"><a class="header" href="#三为什么百分位计算特殊">三、为什么百分位计算特殊？</a></h2>
<p>当我们需要计算一些统计数据时候，大多数统计值可以很直观得到结果，例如: 最值(<code>max/min</code>)、均值(<code>avg</code>)、计数(<code>count</code>)，但是百分位无法预聚合。</p>
<h3 id="31-流式计算"><a class="header" href="#31-流式计算">3.1 流式计算</a></h3>
<p>当计算一个不断新增的数据集(<code>cost_time_list</code>),的最大值(<code>max_cost_time</code>) 时候，我们可以在内存中初始化一个变量(<code>max_value</code>)，表示最大值，随着数据的不断新增，该值(<code>max_value</code>)也会不断变化；但是，当计算该数据集的百分位值，例如: <code>P99</code> ，需要内存中保留所有先前的 <code>cost_time</code> 信息，要维护一个排序的列表 <code>sort_cost_time_list</code> , 通过计算 <code>99%</code> 的 <code>index</code> 值，得到 <code>cost_time</code> 具体的 <code>P99</code> 值。</p>
<p>想象一下，如果数据集有百万/千万，有序列表 <code>sort_cost_time_list</code> 将占用大量的内存。</p>
<h3 id="32-分布式计算"><a class="header" href="#32-分布式计算">3.2 分布式计算</a></h3>
<p>当计算一个分布式数据集的最大值时候，可以先计算出各个独立部分的最大值，然后再次比较这些最大值，可以得到最终的最大值。</p>
<p><img src="https://github.com/Kua-Fu/blog-book-images/blob/main/docs/dataflux_percentile/get_max_value.png?raw=true" alt="最大值" /></p>
<p>但是，我们无法通过简单计算每个子集的百分位值，获取到最终整个数据集的百分位值。</p>
<p><img src="https://github.com/Kua-Fu/blog-book-images/blob/main/docs/dataflux_percentile/get_p99_error.png?raw=true" alt="百分位" /></p>
<p>想要获取到整个数据集的百分位值，需要得到整个数据集的有序列表。这样的方式，我们无法充分利用分布式性能。</p>
<p><img src="https://github.com/Kua-Fu/blog-book-images/blob/main/docs/dataflux_percentile/get_p99_value.png?raw=true" alt="百分位计算" /></p>
<div style="break-before: page; page-break-before: always;"></div><p><meta http-equiv="REFRESH" content="0;url=https://www.poetries.cn/elastic-101/"></HEAD></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asynq"><a class="header" href="#asynq">asynq</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dropbox-atf"><a class="header" href="#dropbox-atf">Dropbox ATF</a></h1>
<h2 id="参考-1"><a class="header" href="#参考-1">参考</a></h2>
<blockquote>
<p><a href="https://dropbox.tech/infrastructure/asynchronous-task-scheduling-at-dropbox">How we designed Dropbox ATF: an async task framework</a></p>
</blockquote>
<blockquote>
<p><a href="https://dropbox.tech/infrastructure/courier-dropbox-migration-to-grpc">Dropbox gRPC</a></p>
</blockquote>
<blockquote>
<p><a href="https://dropbox.tech/infrastructure/reintroducing-edgestore">Dropbox edgeStore</a></p>
</blockquote>
<blockquote>
<p><a href="https://dropbox.tech/infrastructure/asynchronous-task-scheduling-at-dropbox">Amazon Simple Queue Service</a></p>
</blockquote>
<p>How we designed Dropbox ATF: an async task framework</p>
<p>我们如何设计一个异步任务框架</p>
<p>I joined Dropbox not long after graduating with a Master’s degree in computer science. Aside from an internship, this was my first big-league engineering job. My team had already begun designing a critical internal service that most of our software would use: It would handle asynchronous computing requests behind the scenes, powering everything from dragging a file into a Dropbox folder to scheduling a marketing campaign.</p>
<p>This Asynchronous Task Framework (ATF) would replace multiple bespoke async systems used by different engineering teams. It would reduce redundant development, incompatibilities, and reliance on legacy software. There were no open-source projects or buy-not-build solutions that worked well for our use case and scale, so we had to create our own. ATF is both an important and interesting challenge, though, so we were happy to design, build and deploy our own in-house service.、</p>
<p>ATF not only had to work well, it had to work well at scale: It would be a foundational building block of Dropbox infrastructure. It would need to handle 10,000 async tasks per second from the start, and be architected for future growth.  It would need to support nearly 100 unique async task types from the start, again with room to grow. There were at least two dozen engineering teams that would want to use it for entirely different parts of our codebase, for many products and services. </p>
<p>As any engineer would, we Googled to see what other companies with mega-scale services had done to handle async tasks. We were disappointed to find little material published by engineers who built supersized async services.</p>
<p>Now that ATF is deployed and currently serving 9,000 async tasks scheduled per second and in use by 28 engineering teams internally, we’re glad to fill that information gap</p>
<p>我在硕士毕业不久后，就加入了Dropbox公司。除了实习，这是我的第一份大公司工作。我所在的团队，开始设计公司内部都会使用的关键内部组件：它将会在后台处理异步请求，为将文件移动到Dropbox文件夹或者准备一个促销活动等服务提供动力。</p>
<p>这个异步任务框架，将取代其他工程技术团队内部使用的定制异步系统。它将减少冗余开发、不兼容性和软件耦合性。没有任何开源项目能够很好的满足我们的用例和规模，所以，我们必须创建自己的异步任务框架。这是一个重要并且有趣的挑战，因此，我们很高兴设计、构建、部署自己的内部服务。</p>
<p>异步任务框架，必须在异步任务规模变大时候，保证稳定性。它将是Dropbox基础组件。一开始，就必须考虑到每秒1万个异步任务处理能力，并且还需要考虑到未来扩展。同样，一开始，就需要考虑有100多种不同类型的异步任务，并且也要考虑未来扩展。至少有24个团队，希望使用我们开发的异步任务框架，应用于更多的产品和服务。</p>
<p>就像任何工程师一样，我们google搜索，查看其他拥有大规模服务的公司，在处理异步任务方面做了什么。但是，我们很失望的发现，有关如何构建超大型异步服务的文章非常少。</p>
<p>现在，我们的异步任务框架，已经可以处理每秒9000多个任务，并且公司内部有28个团队在使用。所以，我们将很高兴填补大规模异步任务框架的空白。下面，我们将详细记录Dropbox的异步任务框架设计工作，为广大工程师设计自己的异步任务框架，提供参考和指南。</p>
<h2 id="一introduction"><a class="header" href="#一introduction">一、Introduction</a></h2>
<p>介绍</p>
<p>Scheduling asynchronous tasks on-demand is a critical capability that powers many features and internal platforms at Dropbox. Async Task Framework (ATF) is the infrastructural system that supports this capability at Dropbox through a callback-based architecture. ATF enables developers to define callbacks, and schedule tasks that execute against these pre-defined callbacks.</p>
<p>Since its introduction over a year ago, ATF has gone on to become an important building block in the Dropbox infrastructure, used by nearly 30 internal teams across our codebase. It currently supports 100+ use cases which require either immediate or delayed task scheduling. </p>
<p>按照需求，异步调度任务，是Dropbox许多功能和内部平台的关键功能。ATF是基于回调的体系结构创建的。ATF使得开发人员可以定义回调函数，当执行异步任务时候，会执行这些预定义的回调函数。</p>
<p>自从推出ATF一年多以来，它已经成为基础设施的重要组成部分。我们的代码库已经有近30个团队在调用。它目前支持100多个需要立即执行或者延迟调度的用例。</p>
<h2 id="二glossary"><a class="header" href="#二glossary">二、Glossary</a></h2>
<p>名词解释</p>
<p>Some basic terms repeatedly used in this post, defined as used in the context of this discussion.</p>
<ul>
<li>
<p>Lambda: A callback implementing business logic.</p>
</li>
<li>
<p>Task: Unit of execution of a lambda. Each asynchronous job scheduled with ATF is a task.</p>
</li>
<li>
<p>Collection: A labeled subset of tasks belonging to a lambda. If send email is implemented as a lambda, then password reset email and marketing email would be collections.</p>
</li>
<li>
<p>Priority: Labels defining priority of execution of tasks within a lambda. </p>
</li>
</ul>
<p>下面是本文将重复使用的一些术语。</p>
<ul>
<li>
<p>Lambda 实现业务逻辑的回调</p>
</li>
<li>
<p>task lambda执行单位，使用ATF调度的每个异步任务都是一个task</p>
</li>
<li>
<p>collection, 属于一个lambda执行的一组标签任务，例如：</p>
<p>如果发送邮件是一个lambda回调，那么，密码重置和市场推广，都会发送邮件，它们是一组 collection.</p>
</li>
<li>
<p>priority, 优先级，定义同一个lambda回调中的任务优先级。</p>
</li>
</ul>
<h2 id="三features"><a class="header" href="#三features">三、Features</a></h2>
<p>特性</p>
<h3 id="31-task-scheduling"><a class="header" href="#31-task-scheduling">3.1 Task scheduling</a></h3>
<p>任务调度</p>
<p>Clients can schedule tasks to execute at a specified time. Tasks can be scheduled for immediate execution, or delayed to fit the use case.</p>
<p>客户端可以指定任务在指定时间执行，异步任务可以安排为立即执行，也可以延迟执行。</p>
<h3 id="32-priority-based-execution"><a class="header" href="#32-priority-based-execution">3.2 Priority based execution</a></h3>
<p>基于优先级的执行</p>
<p>Tasks should be associated with a priority. Tasks with higher priority should get executed before tasks with a lower priority once they are ready for execution.</p>
<p>每个任务都和优先级关联，优先级高的任务首先执行，优先级低的任务后执行。</p>
<h3 id="33-task-gating"><a class="header" href="#33-task-gating">3.3 Task gating</a></h3>
<p>任务控制</p>
<p>ATF enables the the gating of tasks based on lambda, or a subset of tasks on a lambda based on collection. Tasks can be gated to be completely dropped or paused until a suitable time for execution.</p>
<p>支持（1） 基于lambda回调的任务控制；（2）基于collection的任务控制</p>
<p>异步任务可以被控制为被删除，或者暂停到合适的时间。</p>
<h3 id="34-track-task-status"><a class="header" href="#34-track-task-status">3.4 Track task status</a></h3>
<p>追踪任务执行状态</p>
<p>Clients can query the status of a scheduled task.</p>
<p>客户端可以查看任务执行状态。</p>
<h2 id="四system-guarantees"><a class="header" href="#四system-guarantees">四、System guarantees</a></h2>
<p>稳定性保证</p>
<h3 id="41-at-least-once-task-execution"><a class="header" href="#41-at-least-once-task-execution">4.1 At-least once task execution</a></h3>
<p>至少执行一次</p>
<p>The ATF system guarantees that a task is executed at least once after being scheduled. Execution is said to be complete once the user-defined callback signals task completion to the ATF system.</p>
<p>ATF系统，保证异步任务至少会执行一次，一旦用户定义的回调函数，发送回任务执行完成信号后，该异步任务状态变更为完成状态。</p>
<h3 id="42-no-concurrent-task-execution"><a class="header" href="#42-no-concurrent-task-execution">4.2 No concurrent task execution</a></h3>
<p>没有并发任务执行</p>
<p>The ATF system guarantees that at most one instance of a task will be actively executing at any given in point.This helps users write their callbacks without designing for concurrent execution of the same task from different locations.</p>
<p>ATF系统保证在任何时间点，异步任务最多只有一个实例在运行中。当用户编写回调函数，不需要考虑同一个任务会被并发调用。</p>
<h3 id="43-isolation"><a class="header" href="#43-isolation">4.3 Isolation</a></h3>
<p>隔离性</p>
<p>Tasks in a given lambda are isolated from the tasks in other lambdas. This isolation spans across several dimensions, including worker capacity for task execution and resource use for task scheduling. Tasks on the same lambda but different priority levels are also isolated in their resource use for task scheduling.</p>
<p>不同lambda回调中的异步任务相互隔离。这种隔离跨越多个纬度，包括任务执行的worker数量，任务调度的资源使用。</p>
<p>相同的lambda中，优先级不同的异步任务，执行时候的资源使用，也会隔离。</p>
<h3 id="44-delivery-latency"><a class="header" href="#44-delivery-latency">4.4 Delivery latency</a></h3>
<p>传递延迟</p>
<p>95% of tasks begin execution within five seconds from their scheduled execution time.</p>
<p>95% 的任务在计划执行时间的5秒钟内，会执行。</p>
<h3 id="45-high-availability-for-task-scheduling"><a class="header" href="#45-high-availability-for-task-scheduling">4.5 High availability for task scheduling</a></h3>
<p>任务调度的高可用性</p>
<p>The ATF service is 99.9% available to accept task scheduling requests from any client.</p>
<p>ATF 服务能保证服务可用性为 99.9%</p>
<h2 id="五lambda-requirements"><a class="header" href="#五lambda-requirements">五、Lambda requirements</a></h2>
<p>回调需求</p>
<p>Following are some restrictions we place on the callback logic (lambda):</p>
<p>下面是回调函数的一些限制</p>
<h3 id="51-idempotence"><a class="header" href="#51-idempotence">5.1 Idempotence</a></h3>
<p>幂等</p>
<p>A single task on a lambda can be executed multiple times within the ATF system. Developers should ensure that their lambda logic and correctness of task execution in clients are not affected by this.</p>
<p>一个异步任务对应的lambda回调可以多次执行。开发人员需要确保，lambda逻辑和客户端任务多次执行，不会影响准确性。</p>
<h3 id="52-resiliency"><a class="header" href="#52-resiliency">5.2 Resiliency</a></h3>
<p>弹性</p>
<p>Worker processes which execute tasks might die at any point during task execution.ATF retries abruptly interrupted tasks, which could also be retried on different hosts.  Lambda owners must design their lambdas such that retries on different hosts do not affect lambda correctness.</p>
<p>执行异步任务的worker可能在任何时间点挂掉。ATF将会重试这些失败的异步任务，当然，可能retry发生在其他主机。Lambda 设计者，需要考虑在不同主机上执行相同的异步任务，不会影响准确性。</p>
<h3 id="53-terminal-state-handling"><a class="header" href="#53-terminal-state-handling">5.3 Terminal state handling</a></h3>
<p>状态处理</p>
<p>ATF retries tasks until they are signaled to be complete from the lambda logic. </p>
<p>ATF重新发送异步任务，一直到它接收到任务完成的信号。客户端可以将任务标记为，完成、终止、重试。lambda设计者需要考虑，合适时候发送任务完成信号，需要避免错误行为，例如：无限次重试。这一点非常重要。</p>
<h2 id="六architecture"><a class="header" href="#六architecture">六、Architecture</a></h2>
<p>架构图</p>
<p><img src="https://github.com/Kua-Fu/blog-book-images/blob/main/atf/dropbox-atf.png?raw=true" alt="dropbox atf" /></p>
<p>In this section, we describe the high-level architecture of ATF and give brief description of its different components. Dropbox uses gRPC for remote calls and our in-house Edgestore to store tasks.</p>
<p>ATF consists of the following components: </p>
<ul>
<li>
<p>Frontend</p>
</li>
<li>
<p>Task Store</p>
</li>
<li>
<p>Store Consumer</p>
</li>
<li>
<p>Queue</p>
</li>
<li>
<p>Controller</p>
</li>
<li>
<p>Executor</p>
</li>
<li>
<p>Heartbeat and Status Controller (HSC)</p>
</li>
</ul>
<p>本小节，将描述ATF架构，以及架构中的具体组件。Dropbox 使用gRPC进行远程调度，使用 Edgestore 用于任务存储。</p>
<p>ATF具体下列组件</p>
<h3 id="61-frontend"><a class="header" href="#61-frontend">6.1 Frontend</a></h3>
<p>前端</p>
<p>This is the service that schedules requests via an RPC interface. The frontend accepts RPC requests from clients and schedules tasks by interacting with ATF’s task store described below.</p>
<p>这是发送异步任务请求的客户端，但同时也是，接收RPC接口发送请求的服务端。前端，接收用户发送的RPC请求，然后，构造异步任务，并且通过和任务存储系统交互，调度异步任务。</p>
<h3 id="62-task-store"><a class="header" href="#62-task-store">6.2 Task Store</a></h3>
<p>任务保存</p>
<p>ATF tasks are stored in and triggered from the task store. The task store could be any generic data store with indexed querying capability. In ATF’s case, We use our in-house metadata store Edgestore to power the task store. More details can be found in the Data Model section below.</p>
<p>ATF的异步任务保存在数据库中。保存任务的数据库，可以是任何具有查询能力的数据库。在我们公司，使用内部自研的Edgestore数据库保存任务。</p>
<h3 id="63-store-consumer"><a class="header" href="#63-store-consumer">6.3 Store Consumer</a></h3>
<p>数据库消费者</p>
<p>The Store Consumer is a service that periodically polls the task store to find tasks that are ready for execution and pushes them onto the right queues, as described in the queue section below. These could be tasks that are newly ready for execution, or older tasks that are ready for execution again because they either failed in a retriable way on execution, or were dropped elsewhere within the ATF system. </p>
<p>Below is a simple walkthrough of the Store Consumer’s function: </p>
<pre><code>repeat every second:

  1. poll tasks ready for execution from task store
  
  2. push tasks onto the right queues
  
  3. update task statuses
  
</code></pre>
<p>The Store Consumer polls tasks that failed in earlier execution attempts. This helps with the at-least-once guarantee that the ATF system provides. More details on how the Store Consumer polls new and previously failed tasks is presented in the Lifecycle of a task section below.</p>
<p>数据库消费者提供服务，定期轮询数据库，查看准备好执行的任务，将它们放入正确的队列中。这些异步任务，可能是，最新准备好执行的任务，也可能是旧任务，这些任务，在之前执行过程中失败，或者其他方式被重新执行。</p>
<p>下面是数据库消费者的简单逻辑</p>
<pre><code>每秒执行

1. 从数据库中查询准备好的任务

2. 将任务推送到正确的队列中

3. 更新任务状态
</code></pre>
<p>数据库消费者会轮询先前执行失败的任务。这是保证ATF至少执行一次的特性的设计。关于，消费者，如何轮询新任务、失败的旧任务，可以继续阅读下面内容。</p>
<h3 id="64-queue"><a class="header" href="#64-queue">6.4 Queue</a></h3>
<p>ATF uses AWS Simple Queue Service (SQS) to queue tasks internally. These queues act as a buffer between the Store Consumer and Controllers (described below). Each &lt;lambda, priority&gt;  pair gets a dedicated SQS queue. The total number of SQS queues used by ATF is #lambdas x #priorities.</p>
<p>ATF使用 aws的简单队列服务，作为内部任务队列。这些队列充当数据库消费者和控制器之间的缓存区。每个  &lt;lambda, priority&gt; 都有一个专有的队列。ATF使用的aws SQS队列数量为 </p>
<p>\[ \sum_{i=0}^n  lambda_i \ast priority_i  \]</p>
<h3 id="65-controller"><a class="header" href="#65-controller">6.5 Controller</a></h3>
<p>控制器</p>
<p>Worker hosts are physical hosts dedicated for task execution. Each worker host has one controller process responsible for polling tasks from SQS queues in a background thread, and then pushing them onto process local buffered queues. The Controller is only aware of the lambdas it is serving and thus polls only the limited set of necessary queues. </p>
<p>The Controller serves tasks from its process local queue as a response to NextWork RPCs. This is the layer where execution level task prioritization occurs. The Controller has different process level queues for tasks of different priorities and can thus prioritize tasks in response to NextWork RPCs.</p>
<p>工作主机是专用于任务执行的物理主机。每个主机，都有一个控制器进程，该控制器进程负责在后台轮询来自aws SQS队列的任务，然后将 SQS队列 的任务，推送到本地的进程缓冲队列。控制器，只知道它正在服务的lambdas, 因此它只会轮询一部分 SQS 队列。</p>
<p>控制器将本地进程队列中的任务，作为对NextWork RPC的响应。这是执行任务优先级排序发生的层。控制层，对于不同优先级的任务，具有不同的进程队列，因此，可以响应Nextwork RPC不同的优先级队列。</p>
<h3 id="66-executor"><a class="header" href="#66-executor">6.6 Executor</a></h3>
<p>执行器</p>
<p>The Executor is a process with multiple threads, responsible for the actual task execution. Each thread within an Executor process follows this simple loop:</p>
<pre><code class="language-python">
while True:
  w = get_next_work()
  do_work(w)
  
</code></pre>
<p>Each worker host has a single Controller process and multiple executor processes. Both the Controller and Executors work in a “pull” model, in which active loops continuously long-poll for new work to be done.</p>
<p>执行器是一个具有多个线程的进程，负责实际任务的执行。执行器中的每个线程都遵循着下面简单的循环</p>
<pre><code class="language-python">
while True:
  w = get_next_work()
  do_work(w)
  
</code></pre>
<p>每个工作主机，只有一个控制器进程，但是有多个执行器进程。控制器、执行器都是pull模式运行，在pull模式下，会不断的轮询，获取需要执行的任务。</p>
<h3 id="67-heartbeat-and-status-controller-hsc"><a class="header" href="#67-heartbeat-and-status-controller-hsc">6.7 Heartbeat and Status Controller (HSC)</a></h3>
<p>心跳和状态控制器</p>
<p>The HSC serves RPCs for claiming a task for execution (ClaimTask), setting task status after execution (SetResults) and heartbeats during task execution (Heartbeat). ClaimTask requests originate from the Controllers in response to NextWork requests. Heartbeat and SetResults requests originate from executor processes during and after task execution. The HSC interacts with the task store to update the task status on the kind of request it receives.</p>
<p>心跳和状态控制器，HSC，服务于RPC，可以用于</p>
<p>（1）声明要执行的任务</p>
<p>（2）设置执行后的任务状态</p>
<p>（3）任务执行期间的心跳</p>
<p>声明要执行的任务，来源于响应Nextwork请求的控制器</p>
<p>设置任务状态、心跳，来源于任务执行期间和任务执行后的执行者进程</p>
<p>HSC和数据库交互，根据接收到的任务状态请求类型，更新任务状态。</p>
<h2 id="七data-model"><a class="header" href="#七data-model">七、Data model</a></h2>
<p>数据模型</p>
<p>ATF uses our in-house metadata store, Edgestore, as a task store. Edgestore objects can be Entities or Associations (assoc), each of which can have user-defined attributes. Associations are used to represent relationships between entities. Edgestore supports indexing only on attributes of associations.</p>
<p>Based on this design, we have two kinds of ATF-related objects in Edgestore. The ATF association stores scheduling information, such as the next scheduled timestamp at which the Store Consumer should poll a given task (either for the first time or for a retry).The ATF entity stores all task related information that is used to track the task state and payload for task execution. We query on associations from the Store Consumer in a pull model to pick up tasks ready for execution.</p>
<p>ATF使用内部自研的edgestore数据库存储任务。edgestore数据库中对象，可以是实体或者关联，每个实体和关联都可以具有用户定义的属性。关联用于表示主体之间的关系。edgestore只支持关联属性的索引。</p>
<p>基于上面描述，在 edgestore中有两种和ATF有关的对象。</p>
<p>（1）ATF关联，存储调度信息，例如：数据库消费者应该轮询给定任务（第一次或者重试）的下一个调度时间戳。</p>
<p>（2）ATF实体，存储任务执行状态和任务其他信息。</p>
<p>在 pull模型中，数据库消费者查询ATF关联，获取准备执行的任务。</p>
<h2 id="八lifecycle-of-a-task"><a class="header" href="#八lifecycle-of-a-task">八、Lifecycle of a task</a></h2>
<p>任务生命周期</p>
<ol>
<li>
<p>Client performs a Schedule RPC call to Frontend with task information, including execution time. </p>
</li>
<li>
<p>Frontend creates Edgestore entity and assoc for the task. </p>
</li>
<li>
<p>When it is time to process the task, Store Consumer pulls the task from Edgestore and pushes it to a related SQS queue. </p>
</li>
<li>
<p>Executor makes NextWork RPC call to Controller, which pulls tasks from the SQS queue, makes a ClaimTask RPC to the HSC and then returns the task to the Executor. </p>
</li>
<li>
<p>Executor invokes the callback for the task. While processing, Executor performs Heartbeat RPC calls to Heartbeat and Status Controller (HSC). Once processing is done, Executor performs TaskStatus RPC call to HSC. </p>
</li>
<li>
<p>Upon getting Heartbeat and TaskStatus RPC calls, HSC updates the Edgestore entity and assoc.</p>
</li>
</ol>
<p>Every state update in the lifecycle of a task is accompanied by an update to the next trigger timestamp in the assoc. This ensures that the Store Consumer pulls the task again if there is no change in state of the task within the next trigger timestamp. This helps ATF achieve its at-least-once delivery guarantee by ensuring that no task is dropped.</p>
<ol>
<li>
<p>用户执行RPC 请求于前端，该请求包含异步任务信息（包括执行时间）</p>
</li>
<li>
<p>前端构造 edgestore保存对象，实体和关联</p>
</li>
<li>
<p>当需要处理任务时候，数据库消费者从 edgestore中获取任务，并将任务推送到相关的aws SQS队列。</p>
</li>
<li>
<p>执行器向控制器，发送Nextwork RPC请求，控制器从SQS队列中提取任务，向 心跳和任务状态控制器HSC 发出 声明要执行的任务 RPC，然后，将任务发送给执行器。</p>
</li>
<li>
<p>执行器调用任务的回调，在执行过程中，执行器对HSC发送心跳RPC，异步任务处理完成后，执行器对HSC发送任务执行状态RPC</p>
</li>
<li>
<p>在获得心跳或者状态变更RPC后，HSC将会更新数据库中的实体和关联，两个对象。</p>
</li>
</ol>
<p>异步任务生命周期的每一个状态更新，都伴随着对 edgestore中的任务对应的关联，中下一个触发时间戳的更新。这确保了，如果下一个触发器时间戳内任务的状态没有变化，则数据库消费者，将再次拉动任务。这有助于ATF实现至少一次执行的保证，确保没有异步任务丢失。</p>
<p>Following are the task entity and association states in ATF and their corresponding timestamp updates:</p>
<table><thead><tr><th>Entity status</th><th>Assoc status</th><th>next trigger timestamp in Assoc</th><th>Comment</th></tr></thead><tbody>
<tr><td>new</td><td>new</td><td>scheduled_timestamp of the task</td><td>Pick up new tasks that are ready.</td></tr>
<tr><td>enqueued</td><td>started</td><td>enqueued_timestamp + enqueue_timeout</td><td>Re-enqueue task if it has been in enqueued state for too long. This can happen if the queue loses data or the controller goes down after polling the queue and before the task is claimed.</td></tr>
<tr><td>claimed</td><td>started</td><td>claimed_timestamp + claim_timeout</td><td>Re-enqueue if task is claimed but never transfered to processing. This can happen if Controller is down after claiming a task. Task status is changed to enqueued after re-enqueue.</td></tr>
<tr><td>processing</td><td>started</td><td>heartbeat_timestamp + heartbeat_timeout</td><td>Re-enqueue if task hasn’t sent heartbeat for too long. This can happen if Executor is down. Task status is changed to enqueued after re-enqueue.</td></tr>
<tr><td>retriable failure</td><td>started</td><td>compute next_timestamp according to backoff logic</td><td>Exponential backoff for tasks with retriable failure.</td></tr>
<tr><td>success</td><td>completed</td><td>N/A</td><td></td></tr>
<tr><td>fatal_failure</td><td>completed</td><td>N/A</td><td></td></tr>
</tbody></table>
<p>下面是任务主体/任务关联，状态变更和时间戳变化流程。</p>
<table><thead><tr><th>实体状态</th><th>关联状态</th><th>关联中的下一次触发器时间戳</th><th>备注</th></tr></thead><tbody>
<tr><td>新建</td><td>新建</td><td>任务的执行时间戳</td><td>选择出准备好的任务</td></tr>
<tr><td>排队</td><td>启动</td><td>排队时间戳+排队超时</td><td>如果任务在排队状态时间太长了，需要重新入队列；如果队列丢失数据，或者控制器在轮询队列之后和任务声明之前停机，则可能会发生排队时间过长</td></tr>
<tr><td>声明</td><td>启动</td><td>声明时间戳+声明超时</td><td>如果任务已经声明，但是一直没有变更为处理中，那么会重新入队列; 如果控制器在声明任务后关闭，则可能会发生这种状况；重新排队后，任务状态更新为排队中</td></tr>
<tr><td>处理中</td><td>启动</td><td>心跳时间+心跳超时</td><td>如果任务长时间未发送心跳信号，需要重新排队，如果执行器关闭，可能会发生这种情况；重新排队后，任务状态更改为排队</td></tr>
<tr><td>失败</td><td>启动</td><td>根据退让逻辑，计算下一次时间戳</td><td>退让逻辑，会指数级变化</td></tr>
<tr><td>成功</td><td>完成</td><td></td><td></td></tr>
<tr><td>致命故障</td><td>完成</td><td></td><td></td></tr>
</tbody></table>
<p>Below is the state machine that defines task state transitions: </p>
<p>下面是，任务状态变化的状态机。</p>
<p><img src="https://github.com/Kua-Fu/blog-book-images/blob/main/atf/dropbox-atf-2.png?raw=true" alt="atf-2" /></p>
<h2 id="九achieving-guarantees"><a class="header" href="#九achieving-guarantees">九、Achieving guarantees</a></h2>
<p>实现保证</p>
<h3 id="91-at-least-once-task-execution"><a class="header" href="#91-at-least-once-task-execution">9.1 At-least-once task execution</a></h3>
<p>至少一次执行</p>
<p>At-least-once execution is guaranteed in ATF by retrying a task until it completes execution (which is signaled by a Success or a FatalFailure state). All ATF system errors are implicitly considered retriable failures, and lambda owners have an option of marking tasks with a RetriableFailure state. Tasks might be dropped from the ATF execution pipeline in different parts of the system through transient RPC failures and failures on dependencies like Edgestore or SQS.These transient failures at different parts of the system do not affect the at-least-once guarantee, though, because of the system of timeouts and re-polling from Store Consumer.</p>
<p>在ATF中，通过重试任务，一直到任务完成或者明确发送失败信号，来保证至少执行一次。所有的ATF系统错误，都可以视为可重试的故障，lambda所有者可以选择将任务标记为可重试故障状态。在系统的不同部分，任务可能因为各种故障，从ATF执行管道中被删除，例如：（1）RPC故障；（2）SQS故障；（3）edgestore数据库故障，等等。但是，系统的这些偶发故障，不会影响ATF至少执行一次保证，因为我们存在任务超时，数据库消费者重试机制。</p>
<h3 id="92-no-concurrent-task-execution"><a class="header" href="#92-no-concurrent-task-execution">9.2 No concurrent task execution</a></h3>
<p>没有并发执行的任务</p>
<p>Concurrent task execution is avoided through a combination of two methods in ATF. First, tasks are explicitly claimed through an exclusive task state (Claimed) before starting execution. Once the task execution is complete, the task status is updated to one of Success, FatalFailure or RetriableFailure.  A task can be claimed only if its existing task state is Enqueued (retried tasks go to the Enqueued state as well once they are re-pushed onto SQS).</p>
<p>However, there might be situations where once a long running task starts execution, its heartbeats might fail repeatedly yet the task execution continues. ATF would retry this task by polling it from the store consumer because the heartbeat timeouts would’ve expired. This task can then be claimed by another worker and lead to concurrent execution. </p>
<p>To avoid this situation, there is a termination logic in the Executor processes whereby an Executor process terminates itself as soon as three consecutive heartbeat calls fail.  Each heartbeat timeout is large enough to eclipse three consecutive heartbeat failures. This ensures that the Store Consumer cannot pull such tasks before the termination logic ends them—the second method that helps achieve this guarantee.</p>
<p>通过ATF中两种方法的组合，避免了并发执行异步任务。首先，在开始执行之前，通过独占任务状态（已声明）显式声明任务。任务执行完成后，状态更新为成功、失败、可重试失败。只有当任务的现有状态为已排队时候，才可以声明任务，重试状态的任务，在重新推送到SQS队列后，也会重新更新为排队状态。</p>
<p>但是，可能存在一种场景，一个长时间运行的任务开始执行，任务心跳重复失败，但是实际上，任务仍然在运行着。ATF将通过数据库消费者，轮询该任务，因为该任务心跳超时。然而，这时候任务实际在运行中，这样，将导致并发运行。</p>
<p>为了避免这种场景，在执行器中存在终止逻辑，即一旦有连续3次心跳调用失败，执行进程需要终止自己。心跳超时的定义需要足够大，需要超过3次连续心跳失败的时间之和。这样，确保了数据库消费者，在执行器终止逻辑启用并且终止自己之前，不需要重新轮询声明任务。</p>
<h3 id="93-isolation"><a class="header" href="#93-isolation">9.3 Isolation</a></h3>
<p>隔离性</p>
<p>Isolation of lambdas is achieved through dedicated worker clusters, dedicated queues, and dedicated per-lambda scheduling quotas. In addition, isolation across different priorities within the same lambda is likewise achieved through dedicated queues and scheduling bandwidth.</p>
<p>不同的lambda的隔离，通过专用工作集群、专用队列、专用的调度配额实现的。同样，相同的lambda，通过专有队列、专有调度配额实现不同优先级的任务。</p>
<h3 id="94-delivery-latency"><a class="header" href="#94-delivery-latency">9.4 Delivery latency</a></h3>
<p>传递延迟</p>
<p>ATF use cases do not require ultra-low task delivery latencies.  Task delivery latencies on the order of a couple of seconds are acceptable. Tasks ready for execution are periodically polled by the Store Consumer and this period of polling largely controls the task delivery latency. Using this as a tuning lever, ATF can achieve different delivery latencies as required.   Increasing poll frequency reduces task delivery latency and vice versa. Currently, we have calibrated ATF to poll for ready tasks once every two seconds.</p>
<p>ATF不适用于，非常低延迟的任务。几秒钟的任务延迟是可以接受的范围。数据库消费者周期性轮询准备执行的任务，这种周期性轮询很大程度上，可以控制任务延迟。将这个周期性轮询当作一个调节，根据需要实现不同的延迟。添加轮询频率，可以减少任务延迟，反之亦然。目前，我们已经设置默认的轮询频率为，每2秒钟，执行一次轮询。</p>
<h2 id="十ownership-model"><a class="header" href="#十ownership-model">十、Ownership model</a></h2>
<p>所有权模式</p>
<p>ATF is designed to be a self-serve framework for developers at Dropbox.  The design is very intentional in driving an ownership model where lambda owners own all aspects of their lambdas’ operations. To promote this, all lambda worker clusters are owned by the lambda owners. They have full control over operations on these clusters, including code deployments and capacity management
Each executor process is bound to one lambda. Owners have the option of deploying multiple lambdas on their worker clusters simply by spawning new executor processes on their hosts.
ATF 旨在成为开发者的自服务框架。这种设计想要推动一种所有权模式，其中，lambda所有者拥有lambda操作的所有配置。为了促进，所有lambda工作集群都由lambda所有者拥有。lambda所有者，完全控制集群上的操作，例如：代码部署，集群资源管理。每个执行器都绑定到一个lambda上，所有者可以选择在工作集群上部署多个lambda，只需要在主机上生成新的执行器。</p>
<h2 id="十一extending-atf"><a class="header" href="#十一extending-atf">十一、Extending ATF</a></h2>
<p>扩展ATF</p>
<p>As described above, ATF provides an infrastructural building block for scheduling asynchronous tasks. With this foundation established, ATF can be extended to support more generic use cases and provide more features as a framework. Following are some examples of what could be built as an extension to ATF. </p>
<p>如上所述，ATF为异步调度任务提供了基础模块。在此基础上，ATF可以扩展支持更加通用的用例，并作为框架提供更多功能。下面是一些示例</p>
<h3 id="111-periodic-task-execution"><a class="header" href="#111-periodic-task-execution">11.1 Periodic task execution</a></h3>
<p>定期执行任务</p>
<p>Currently, ATF is a system for one-time task scheduling. Building support for periodic task execution as an extension to this framework would be useful in unlocking new capabilities for our clients.</p>
<p>目前，ATF是一个用于一次性任务调度的系统，构建对于定期任务的支持，作为该框架的扩展，将有助于为客户端解锁新功能。</p>
<h3 id="112-better-support-for-task-chaining"><a class="header" href="#112-better-support-for-task-chaining">11.2 Better support for task chaining</a></h3>
<p>更好的支持任务链</p>
<p>Currently, it is possible to chain tasks on ATF by scheduling a task onto ATF that then schedules other tasks onto ATF during its execution. Although it is possible to do this in the current ATF setup, visibility and control on this chaining is absent at the framework level.Another natural extension here would be to better support task chaining through framework-level visibility and control, to make this use case a first class concept in the ATF model.</p>
<p>现在，可以在一个任务被执行时候，ATF调度另外任务，实现一种任务链。虽然，在当前的ATF设置中，可以做到这一点，但是在框架级别上，可以任务链的可见性和控制是不存在的。这里的另一个自然扩展是，通过框架级别的可见性、控制，更好的支持任务链，在ATF模型中，将任务链作为第一类对象。</p>
<h3 id="113-dead-letter-queues-for-misbehaving-tasks"><a class="header" href="#113-dead-letter-queues-for-misbehaving-tasks">11.3 Dead letter queues for misbehaving tasks</a></h3>
<p>One common source of maintenance overhead we observe on ATF is that some tasks get stuck in infinite retry loops due to occasional bugs in lambda logic. This requires manual intervention from the ATF framework owners in some cases where there are a large number of tasks stuck in such loops, occupying a lot of the scheduling bandwidth in the system. Typical manual actions in response to such a situation include pausing execution of the lambdas with misbehaving tasks, or dropping them outright.</p>
<p>One way to reduce this operational overhead and provide an easy interface for lambda owners to recover from such incidents would be to create dead letter queues filled with such misbehaving tasks. The ATF framework could impose a maximum number of retries before tasks are pushed onto the dead letter queue. We could create and expose tools that make it easy to reschedule tasks from the dead letter queue back into the ATF system, once the associated lambda bugs are fixed.</p>
<p>当我们观测ATF，一个常见的资源开销来源是，由于lambda逻辑中存在错误，导致一些任务陷入无限循环。这需要ATF框架使用者，手动干预，在某些极端场景，大量任务卡在无限循环中，占用了大量的系统资源。处理这种问题的典型操作是，手动暂停执行具有错误行为的任务的lambda，或者直接删除这些lambda。</p>
<p>减少手动操作并且为lambda所有者提供从此类事件中恢复的简单接口的，一种可行方式是，创建一个队列，里面包含无限循环错误的任务。ATF框架，在将任务转移到新队列前，多次重试该任务，如果lambda被修复，可以再将新队列中的任务，转移回ATF框架中。</p>
<h2 id="十二conclusion"><a class="header" href="#十二conclusion">十二、Conclusion</a></h2>
<p>结论</p>
<p>We hope this post helps engineers elsewhere to develop better async task frameworks of their own. Many thanks to everyone who worked on this project: Anirudh Jayakumar, Deepak Gupta, Dmitry Kopytkov, Koundinya Muppalla, Peng Kang, Rajiv Desai, Ryan Armstrong, Steve Rodrigues, Thomissa Comellas, Xiaonan Zhang and Yuhuan Du.</p>
<p>我们希望这篇文章可以帮助更多的工程师，构建自己的异步任务框架。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
